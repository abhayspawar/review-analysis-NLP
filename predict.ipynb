{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/gensim/utils.py:1015: UserWarning: Pattern library is not installed, lemmatization won't be available.\n",
      "  warnings.warn(\"Pattern library is not installed, lemmatization won't be available.\")\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "import time\n",
    "import collections\n",
    "from nltk.corpus import stopwords\n",
    "import nltk.data\n",
    "import networkx as nx\n",
    "import proj_base\n",
    "from gensim.models import word2vec\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.spatial.distance as scpd\n",
    "#tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(124, 13)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = proj_base.getStandardData(numFiles=1)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sentences = []\n",
    "\n",
    "#word2vec takes a list of lists, where each internal list is a BOW\n",
    "\n",
    "\n",
    "def review2sentences(review, tokenizer, remove_stopwords=True):\n",
    "    #split each review into sentences\n",
    "    raw_sent = tokenizer.tokenize(review.strip())\n",
    "    \n",
    "    sentences = []\n",
    "    for s in raw_sent:\n",
    "        if len(s) > 0 :\n",
    "            sentences.append(review2wordlist(s, remove_stopwords))\n",
    "            \n",
    "    return sentences\n",
    "\n",
    "\n",
    "def review2wordlist(review_text, remove_stopwords=True):\n",
    "    #split the given text into BOW\n",
    "    \n",
    "\n",
    "    review_text = re.sub(\"[^a-zA-Z]\",\" \", review_text)\n",
    "\n",
    "    words = review_text.lower().split()\n",
    "    if remove_stopwords:\n",
    "        stops = set(stopwords.words(\"english\"))\n",
    "        words = [w for w in words if not w in stops]\n",
    "\n",
    "    return(words)\n",
    "\n",
    "\n",
    "for review in data[\"Content\"]:\n",
    "    sentences += review2sentences(review, tokenizer)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1246"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training\n"
     ]
    }
   ],
   "source": [
    "#BUILD WORD2Vec infrastructure\n",
    "\n",
    "num_features = 100\n",
    "min_word_count = 40\n",
    "num_workers = 2\n",
    "context = 2\n",
    "downsampling = 1e-3\n",
    "\n",
    "from gensim.models import word2vec\n",
    "print(\"training\")\n",
    "model = word2vec.Word2Vec(sentences, workers=num_workers, size=num_features)\n",
    "\n",
    "def makeFeatureVec(words, model, num_features):\n",
    "    #given a list of words and the pre-trained word model return a feature vector\n",
    "    featureVec = np.zeros((num_features,), dtype=\"float32\")\n",
    "    \n",
    "    nwords = 0\n",
    "    index2word_set = set(model.index2word)\n",
    "    \n",
    "    \n",
    "    #get the mean vector for each review\n",
    "    for w in words:\n",
    "        #print(w)\n",
    "        if w in index2word_set:\n",
    "            nwords = nwords + 1\n",
    "            featureVec = np.add(featureVec, model[w])\n",
    "            \n",
    "    featureVec = np.divide(featureVec, nwords)\n",
    "    return featureVec\n",
    "\n",
    "def getAvgFeatureVec(reviews, model, num_features):\n",
    "    #get average feature vec for list of bag of words\n",
    "    count = 0\n",
    "    reviewFeatureVec = np.zeros((len(reviews), num_features), dtype=\"float32\")\n",
    "    \n",
    "    for r in reviews:\n",
    "        if count % 1000 == 0:\n",
    "            print(\"at review\", count)\n",
    "        #for each review add the feature vec\n",
    "        reviewFeatureVec[count] = makeFeatureVec(r, model, num_features)\n",
    "        count += 1\n",
    "        \n",
    "    return reviewFeatureVec\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['wonderful', 'time', 'even', 'snow', 'great', 'experience', 'goldfish', 'room', 'daughter', 'loved', 'fact', 'valet', 'parking', 'staff', 'put', 'chains', 'fabulous', 'staff', 'attentive', 'went', 'beyond', 'make', 'stay', 'enjoyable', 'oh', 'parking', 'charge', 'would', 'pay', 'garage', 'lot', 'bet', 'help', 'snow']\n",
      "[ -3.48742306e-02  -9.19236988e-03  -2.74260025e-02  -2.39881426e-02\n",
      "   6.74560945e-03  -1.78331602e-02   2.67210254e-03   1.17838674e-03\n",
      "   2.38685757e-02   1.77801996e-02   1.30458809e-02  -5.14267804e-03\n",
      "   1.03308987e-02   3.24241817e-03  -1.80550071e-03   8.11703317e-03\n",
      "  -2.68183323e-03  -6.06593443e-03  -6.57466352e-02  -2.84223892e-02\n",
      "   1.06701739e-02  -8.60380661e-03  -8.69145896e-03   3.47222248e-03\n",
      "   1.90774053e-02  -1.78701840e-02  -2.40057129e-02  -2.68744468e-03\n",
      "  -1.19224424e-02   2.31881607e-02  -8.37193243e-03  -1.82689633e-02\n",
      "   3.32673488e-04   4.54220846e-02   2.12338064e-02  -6.63632760e-03\n",
      "   1.22047374e-02  -1.93763711e-02  -4.00290042e-02   3.00723780e-03\n",
      "   2.74695586e-02  -1.89196728e-02   6.65924372e-03  -2.45029368e-02\n",
      "   2.80418899e-02   4.25090268e-02   1.42577216e-02   1.38600813e-02\n",
      "  -1.90658998e-02  -1.57720111e-02   6.83445670e-03  -1.21733351e-02\n",
      "  -4.13428387e-03   7.15898210e-03   8.74416553e-04   6.85887598e-03\n",
      "   2.54286062e-02   9.87792481e-03  -1.54101690e-02   1.67501643e-02\n",
      "   1.85135822e-03   2.32305937e-03  -3.18770595e-02   1.78286955e-02\n",
      "  -5.31827519e-03   2.78469571e-03   2.08890089e-03  -8.97088461e-03\n",
      "   2.20327657e-02   1.04897451e-02   1.08855739e-02   3.27407219e-03\n",
      "   7.68543407e-03   4.00264573e-04   2.58875154e-02   1.44538162e-02\n",
      "  -1.92937162e-03   2.59238090e-02   6.83535589e-03   3.38911705e-05\n",
      "  -3.24857160e-02   3.58798541e-02   1.80498920e-02  -7.66024971e-03\n",
      "   4.40270491e-02  -2.04412937e-02   4.88347234e-03  -1.14036687e-02\n",
      "   1.97705161e-02  -8.58644769e-03   3.64356302e-02  -1.76332444e-02\n",
      "   1.66623835e-02   1.41941067e-02   1.72438957e-02   1.83589738e-02\n",
      "   1.63959991e-02  -1.05601549e-02   2.31527314e-02  -2.48516467e-03]\n"
     ]
    }
   ],
   "source": [
    "rev1 = data[\"Content\"][0]\n",
    "rev12words = review2wordlist(rev1, tokenizer)\n",
    "print(rev12words)\n",
    "rev1avg = makeFeatureVec(rev12words, model, num_features)\n",
    "print(rev1avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#GET HARD TRUTH RATINGS\n",
    "\n",
    "aspect=\"Location\"\n",
    "\n",
    "def filterHardTruthForAspect(data, aspect):\n",
    "    #give data points that have hard truth ratings\n",
    "    return  data[pd.notnull(data[aspect])]\n",
    "\n",
    "justLoc = filterHardTruthForAspect(data, aspect)\n",
    "justLoc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "def buildGraph(data, sim):\n",
    "    \n",
    "    G = nx.Graph()\n",
    "    neighborsForNodes = 6\n",
    "    nodes = data.shape[0]\n",
    "    # build nodes\n",
    "    for i in range(nodes):\n",
    "        rev = data.loc[i]\n",
    "        content = rev[\"Content\"]\n",
    "        #todo only add relevant sentences or take whole content?\n",
    "        \n",
    "        G.add_node(i, {'author' : rev[\"Author\"], 'content' :content})\n",
    "        \n",
    "        #if i has a hard-truth rating add a node with that dongle\n",
    "        if rev[aspect] != np.nan:\n",
    "            G.add_node(i+nodes, truth=True, rating=rev[aspect])\n",
    "            G.add_edge(i, i+nodes, weight=1)\n",
    "    \n",
    "    # build edges\n",
    "    sims = buildSimilarityMatrix(data)\n",
    "    for i in range(nodes):\n",
    "        kthClosest = sorted(sims[i], reverse=True)[5]\n",
    "        for j in range(nodes):\n",
    "            thisSimilar = sims[i][j]\n",
    "            if thisSimilar > kthClosest:\n",
    "                G.add_edge(i, j, weight=thisSimilar)\n",
    "    \n",
    "    \n",
    "\n",
    "    # add separate learner scores\n",
    "    \n",
    "    return G\n",
    "\n",
    "def howSimilar(r1, r2):\n",
    "    #use word2vec cosine similarity\n",
    "    \n",
    "    rev2words = review2wordlist(r1, tokenizer)\n",
    "    rev1vec = makeFeatureVec(rev2words, model, num_features)\n",
    "    revj2words = review2wordlist(r2)\n",
    "    rev2vec = makeFeatureVec(revj2words, model, num_features)\n",
    "    return scpd.cosine(rev1vec, rev2vec)\n",
    "\n",
    "\n",
    "def buildSimilarityMatrix(data):\n",
    "    \n",
    "    similar = [[0]*nodes for i in range(nodes)]\n",
    "    \n",
    "    for i in range(nodes):\n",
    "        dati = data.loc[i][\"Content\"]\n",
    "        \n",
    "        for j in range(nodes):\n",
    "            if i != j:\n",
    "                datj = data.loc[j][\"Content\"]\n",
    "               \n",
    "                similar[i][j] = howSimilar(dati, data.loc[j][\"Content\"])\n",
    "                \n",
    "    return similar\n",
    "\n",
    "\n",
    "\n",
    "similarities = buildSimilarityMatrix(data)\n",
    "g = buildGraph(data, similarities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rev2words = review2wordlist(data[\"Content\"][0], tokenizer)\n",
    "rev1vec = makeFeatureVec(rev2words, model, num_features)\n",
    "revj2words = review2wordlist(data[\"Content\"][1])\n",
    "rev2vec = makeFeatureVec(revj2words, model, num_features)\n",
    "scpd.cosine(rev1vec, rev2vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def getLoss(graph):\n",
    "    #minimize loss ratings over the graph\n",
    "    a = 2\n",
    "    b = 1\n",
    "    M = 10000  #strength of hard truth\n",
    "    \n",
    "    #for edge in graph\n",
    "    #   loss = sum_labeled[M*(pred_i - actual_i)^2 ] + sum_unlabeled[(pred_i - learner_i)^2] + \\\\\n",
    "    #   + sum_unlabeled[sum_knnLabeled[a*w_i_j*(pred_i-pred_j)^2]]  + \\\\\n",
    "    #   + sum_unlabeled[sum_knnUnlabled[b*w_i_j*(pred_i-pred_j)^2]]\n",
    "    \n",
    "    \n",
    "    return loss\n",
    "\n",
    "def smoothness(edge):\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "nx.draw(g)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#from sklearn.svm import SVC\n",
    "\n",
    "#do svm classification\n",
    "#need to split data into hard-truth train and predict\n",
    "#not sure how were actually going to do this. Hard-truth ratings are needed to train the model right?\n",
    "#but we'll also need to have some to test the results\n",
    "#so probably going to need to have a lot of reviews\n",
    "\n",
    "#train = sample of data with hard rating\n",
    "\n",
    "trainDataVecs = getAvgFeatureVec(train, model, num_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
