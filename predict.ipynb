{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/gensim/utils.py:1015: UserWarning: Pattern library is not installed, lemmatization won't be available.\n",
      "  warnings.warn(\"Pattern library is not installed, lemmatization won't be available.\")\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "import time\n",
    "import collections\n",
    "from nltk.corpus import stopwords\n",
    "import nltk.data\n",
    "import networkx as nx\n",
    "import proj_base\n",
    "from gensim.models import word2vec\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.spatial.distance as scpd\n",
    "from numpy.linalg import inv\n",
    "\n",
    "import time\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "\n",
    "#tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1111, 13)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = proj_base.getStandardData(numFiles=10)\n",
    "unchanged = data.copy()\n",
    "aspect = \"Location\"\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training\n"
     ]
    }
   ],
   "source": [
    "sentences = []\n",
    "\n",
    "#word2vec takes a list of lists, where each internal list is a BOW\n",
    "\n",
    "\n",
    "def review2sentences(review, tokenizer, remove_stopwords=True):\n",
    "    #split each review into sentences\n",
    "    raw_sent = tokenizer.tokenize(review.strip())\n",
    "    \n",
    "    sentences = []\n",
    "    for s in raw_sent:\n",
    "        if len(s) > 0 :\n",
    "            sentences.append(review2wordlist(s, remove_stopwords))\n",
    "            \n",
    "    return sentences\n",
    "\n",
    "\n",
    "def review2wordlist(review_text, remove_stopwords=True):\n",
    "    #split the given text into BOW\n",
    "    \n",
    "\n",
    "    review_text = re.sub(\"[^a-zA-Z]\",\" \", review_text)\n",
    "\n",
    "    words = review_text.lower().split()\n",
    "    if remove_stopwords:\n",
    "        stops = set(stopwords.words(\"english\"))\n",
    "        words = [w for w in words if not w in stops]\n",
    "\n",
    "    return(words)\n",
    "\n",
    "\n",
    "def sentences2wordlist(sentences, remove_stopwords=True):\n",
    "    \n",
    "    allWords = []\n",
    "    for s in sentences: \n",
    "        s = re.sub(\"[^a-zA-Z]\",\" \", s)\n",
    "        words = s.lower().split()\n",
    "        if remove_stopwords:\n",
    "            stops = set(stopwords.words(\"english\"))\n",
    "            allWords.extend([w for w in words if not w in stops])\n",
    "\n",
    "    return(allWords)\n",
    "\n",
    "for review in data[\"Content\"]:\n",
    "    sentences += review2sentences(review, tokenizer)\n",
    "    \n",
    "\n",
    "#BUILD WORD2Vec infrastructure\n",
    "\n",
    "num_features = 100\n",
    "min_word_count = 40\n",
    "num_workers = 2\n",
    "context = 2\n",
    "downsampling = 1e-3\n",
    "\n",
    "from gensim.models import word2vec\n",
    "print(\"training\")\n",
    "model = word2vec.Word2Vec(sentences, workers=num_workers, size=num_features)\n",
    "\n",
    "def makeFeatureVec(words, model, num_features):\n",
    "    #given a list of words and the pre-trained word model return a feature vector\n",
    "    featureVec = np.zeros((num_features,), dtype=\"float32\")\n",
    "    \n",
    "    nwords = 0\n",
    "    index2word_set = set(model.index2word)\n",
    "    \n",
    "    \n",
    "    #get the mean vector for each review\n",
    "    for w in words:\n",
    "        #print(w)\n",
    "        if w in index2word_set:\n",
    "            nwords = nwords + 1\n",
    "            featureVec = np.add(featureVec, model[w])\n",
    "            \n",
    "    featureVec = np.divide(featureVec, nwords)\n",
    "    return featureVec\n",
    "\n",
    "def getAvgFeatureVec(reviews, model, num_features):\n",
    "    #get average feature vec for list of bag of words\n",
    "    count = 0\n",
    "    reviewFeatureVec = np.zeros((len(reviews), num_features), dtype=\"float32\")\n",
    "    \n",
    "    for r in reviews:\n",
    "        if count % 1000 == 0:\n",
    "            print(\"at review\", count)\n",
    "        #for each review add the feature vec\n",
    "        reviewFeatureVec[count] = makeFeatureVec(r, model, num_features)\n",
    "        count += 1\n",
    "        \n",
    "    return reviewFeatureVec\n",
    "\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['daughter', 'th', 'birthday', 'promised', 'take', 'two', 'girlfriends', 'see', 'shrek', 'musical', 'th', 'avenue', 'theater', 'decided', 'make', 'fun', 'night', 'booking', 'nearby', 'hotel', 'well', 'settled', 'monaco', 'based', 'proximity', 'theater', 'price', 'trip', 'adviser', 'reviews', 'rewarding', 'experience', 'play', 'started', 'pm', 'day', 'called', 'requested', 'early', 'check', 'told', 'guarantee', 'industry', 'standard', 'left', 'house', 'headed', 'monaco', 'real', 'expectation', 'able', 'check', 'room', 'early']\n"
     ]
    }
   ],
   "source": [
    "print(sentences2wordlist(data.loc[1][\"aspectSentences\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('time taken', 9.512901306152344e-05)\n"
     ]
    }
   ],
   "source": [
    "def isFeatureVecNull(fv):\n",
    "    if sum(pd.notnull(fv)) == len(fv):\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def getFeatureVec(review, useAspectSentences=False):\n",
    "    if useAspectSentences:\n",
    "        rev2words = sentences2wordlist(review)\n",
    "    else:\n",
    "        rev2words = review2wordlist(review, tokenizer)\n",
    "    return makeFeatureVec(rev2words, model, num_features)\n",
    "\n",
    "\n",
    "def getAllSims(rev, allRevs):\n",
    "    return allRevs.apply(scpd.cosine, args=(rev,))\n",
    "\n",
    "def buildSimilarityMatrix(data):\n",
    "    numNodes = data.shape[0]\n",
    "    sims = data[\"featureVec\"].apply(getAllSims, args=(data[\"featureVec\"],))\n",
    "    \n",
    "  \n",
    "                \n",
    "    return sims\n",
    "\n",
    "time1 = time.time()\n",
    "\n",
    "#similarities = buildSimilarityMatrix(data)\n",
    "#g = buildGraph(data, similarities)\n",
    "time2 = time.time()\n",
    "\n",
    "print(\"time taken\", time2-time1)\n",
    "#similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.9380903279181352"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.norm(similarities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def calcMinLossPredictions(data):\n",
    "    #define constants\n",
    "    sim = buildSimilarityMatrix(data)\n",
    "    sim = (sim - sim.mean()) / (sim.max() - sim.min())\n",
    "    n = data.shape[0]\n",
    "    print(\"Num data\", n)\n",
    "    a = 3.5\n",
    "    b = 1.5\n",
    "    k = 5\n",
    "    k_p = 3\n",
    "    M = 700\n",
    "    \n",
    "    isLabeled = np.isnan(data[aspect]) == False \n",
    "    \n",
    "    y = np.maximum(np.nan_to_num(data[aspect]), np.ones(len(data)))\n",
    "    \n",
    "    kNN = np.zeros((n, n), float)\n",
    "    #build kNN matrix\n",
    "    for i in range(n):\n",
    "        kthClosest = sorted(sim[i], reverse=True)[k]\n",
    "        for j in range(n):\n",
    "            thisSimilar = sim[i][j]\n",
    "            if thisSimilar >= kthClosest:\n",
    "                kNN[i][j] = 1\n",
    "        \n",
    "    \n",
    "    #build matricies\n",
    "    \n",
    "    D = np.zeros((n, n), float)\n",
    "    d_diag = np.zeros(n)\n",
    "    for i in range(n):\n",
    "        d_diag[i] =  sum(sorted(sim[i], reverse=True)[0:k])\n",
    "    np.fill_diagonal(D, d_diag)\n",
    "    \n",
    "    c_diag = np.maximum(np.nan_to_num(np.maximum(data[aspect], np.ones(len(data))*M)), np.ones(len(data)))\n",
    "    C = np.zeros((n, n), int)\n",
    "    np.fill_diagonal(C, c_diag)\n",
    "    \n",
    "    W_p = np.zeros((n,n), float)\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            if isLabeled[i]:\n",
    "                W_p[i][j] = 0\n",
    "            elif isLabeled[i] and kNN[i][j] == 1:\n",
    "                W_p[i][j] = sim[i][j]\n",
    "            elif not isLabeled[i] and kNN[i][j] == 1:\n",
    "                W_p[i][j] = b*sim[i][j]\n",
    "                \n",
    "    W = np.maximum(W_p, np.transpose(W_p))\n",
    "    \n",
    "    delta = D - W\n",
    "    \n",
    "    constant = a*1.0/(k + k_p*b)\n",
    "    toInv = C + constant*delta\n",
    "    inverse = inv(toInv)\n",
    "    \n",
    "    C_y = C.dot(y)\n",
    "    preds = inverse.dot(C_y)\n",
    "    \n",
    "    \n",
    "    return np.round(np.clip(preds, 1, 5))\n",
    "\n",
    "\n",
    "#preds = calcMinLossPredictions(data)\n",
    "#preds\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('aspectRating', (812, 5))\n"
     ]
    }
   ],
   "source": [
    "data = proj_base.getStandardData(numFiles=33)\n",
    "unchanged = data.copy()\n",
    "aspect = \"Location\"\n",
    "data = proj_base.getTrainingData(data)\n",
    "data = data.reset_index()\n",
    "aspect = \"aspectRating\"\n",
    "unchanged = data.copy()\n",
    "\n",
    "print(aspect, data.shape)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Num data', 800)\n",
      "DONE\n",
      "0.23\n"
     ]
    }
   ],
   "source": [
    "numTrain = 700\n",
    "numTest = 100\n",
    "dat = data[0:numTrain]\n",
    "\n",
    "\n",
    "\n",
    "#split data into train and test\n",
    "moreData = data[numTrain:numTrain+numTest]\n",
    "truth = unchanged[aspect][numTrain:numTrain+numTest]\n",
    "moreData[aspect] = np.nan  #set last 10 to no prediction\n",
    "fulldat = pd.concat([dat, moreData])\n",
    "\n",
    "#remove reviews where feature vec is null\n",
    "\n",
    "#use full content or aspect sentences?\n",
    "fulldat[\"featureVec\"] = fulldat[\"Content\"].apply(getFeatureVec)\n",
    "#fulldat[\"featureVec\"] = fulldat[\"aspectSentences\"].apply(getFeatureVec, args=(True,))\n",
    "fvNull = fulldat[\"featureVec\"].apply(isFeatureVecNull)\n",
    "fulldat = fulldat[fvNull]\n",
    "\n",
    "\n",
    "fullpreds = calcMinLossPredictions(fulldat)\n",
    "\n",
    "print(\"DONE\")\n",
    "print(np.mean(fullpreds[numTrain:numTrain+numTest] == truth))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.20000000000000001"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(fullpreds[numTrain:numTrain+numTest] == truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('pred v truth:', 5.0, 5.0)\n",
      "('pred v truth:', 5.0, 4.0)\n",
      "('pred v truth:', 5.0, 5.0)\n",
      "('pred v truth:', 5.0, 5.0)\n",
      "('pred v truth:', 5.0, 5.0)\n",
      "('pred v truth:', 5.0, 3.0)\n",
      "('pred v truth:', 5.0, 3.0)\n",
      "('pred v truth:', 5.0, 5.0)\n",
      "('pred v truth:', 5.0, 2.0)\n",
      "('pred v truth:', 5.0, 3.0)\n",
      "('pred v truth:', 5.0, 5.0)\n",
      "('pred v truth:', 5.0, 4.0)\n",
      "('pred v truth:', 5.0, 1.0)\n",
      "('pred v truth:', 5.0, 5.0)\n",
      "('pred v truth:', 5.0, 5.0)\n",
      "('pred v truth:', 4.0, 5.0)\n",
      "('pred v truth:', 5.0, 4.0)\n",
      "('pred v truth:', 5.0, 5.0)\n",
      "('pred v truth:', 5.0, 5.0)\n",
      "('pred v truth:', 4.0, 5.0)\n",
      "('pred v truth:', 5.0, 5.0)\n",
      "('pred v truth:', 4.0, 5.0)\n",
      "('pred v truth:', 5.0, 5.0)\n",
      "('pred v truth:', 4.0, 5.0)\n",
      "('pred v truth:', 5.0, 5.0)\n",
      "('pred v truth:', 5.0, 4.0)\n",
      "('pred v truth:', 5.0, 3.0)\n",
      "('pred v truth:', 5.0, 4.0)\n",
      "('pred v truth:', 5.0, 5.0)\n",
      "('pred v truth:', 5.0, 5.0)\n"
     ]
    }
   ],
   "source": [
    "predicted = fullpreds[numTrain:numTrain+numTest]\n",
    "\n",
    "for p in range(len(truth)):\n",
    "    print(\"pred v truth:\",predicted[p], truth[numTrain+p])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "700    5.0\n",
       "701    5.0\n",
       "702    5.0\n",
       "703    4.0\n",
       "704    4.0\n",
       "705    4.0\n",
       "706    4.0\n",
       "707    4.0\n",
       "708    3.0\n",
       "709    1.0\n",
       "710    5.0\n",
       "711    4.0\n",
       "712    2.0\n",
       "713    4.0\n",
       "714    2.0\n",
       "715    5.0\n",
       "716    3.0\n",
       "717    5.0\n",
       "718    3.0\n",
       "719    1.0\n",
       "720    3.0\n",
       "721    2.0\n",
       "722    4.0\n",
       "723    5.0\n",
       "724    2.0\n",
       "725    2.0\n",
       "726    4.0\n",
       "727    2.0\n",
       "728    2.0\n",
       "729    2.0\n",
       "      ... \n",
       "740    1.0\n",
       "741    3.0\n",
       "742    5.0\n",
       "743    5.0\n",
       "744    4.0\n",
       "745    5.0\n",
       "746    3.0\n",
       "747    2.0\n",
       "748    4.0\n",
       "749    4.0\n",
       "750    3.0\n",
       "751    5.0\n",
       "752    3.0\n",
       "753    5.0\n",
       "754    5.0\n",
       "755    5.0\n",
       "756    5.0\n",
       "757    4.0\n",
       "758    2.0\n",
       "759    4.0\n",
       "760    5.0\n",
       "761    3.0\n",
       "762    5.0\n",
       "763    5.0\n",
       "764    4.0\n",
       "765    2.0\n",
       "766    4.0\n",
       "767    4.0\n",
       "768    4.0\n",
       "769    2.0\n",
       "Name: aspectRating, dtype: float64"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# def buildGraph(data, sims):\n",
    "    \n",
    "#     G = nx.Graph()\n",
    "#     neighborsForNodes = 6\n",
    "#     nodes = data.shape[0]\n",
    "#     # build nodes\n",
    "#     for i in range(nodes):\n",
    "#         rev = data.loc[i]\n",
    "#         content = rev[\"Content\"]\n",
    "#         #todo only add relevant sentences or take whole content?\n",
    "#         hasHardTruth = pd.notnull(rev[aspect])\n",
    "#         G.add_node(i, {'author' : rev[\"Author\"], 'content' :content, \"labeled\": hasHardTruth, 'truth':False})\n",
    "        \n",
    "#         #if i has a hard-truth rating add a node with that dongle\n",
    "#         if hasHardTruth:\n",
    "#             #print(rev[aspect])\n",
    "#             G.add_node(i+nodes, truth=True, rating=rev[aspect], dongle=True)\n",
    "#             G.add_edge(i, i+nodes, weight=1)\n",
    "    \n",
    "#     # build edges\n",
    "#     #sims = buildSimilarityMatrix(data)\n",
    "#     for i in range(nodes):\n",
    "#         kthClosest = sorted(sims[i], reverse=True)[5]\n",
    "#         for j in range(nodes):\n",
    "#             thisSimilar = sims[i][j]\n",
    "#             if thisSimilar > kthClosest:\n",
    "#                 G.add_edge(i, j, weight=thisSimilar)\n",
    "    \n",
    "    \n",
    "\n",
    "#     # add separate learner scores\n",
    "    \n",
    "#     return G"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
