{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/gensim/utils.py:1015: UserWarning: Pattern library is not installed, lemmatization won't be available.\n",
      "  warnings.warn(\"Pattern library is not installed, lemmatization won't be available.\")\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "import copy\n",
    "\n",
    "import collections\n",
    "from operator import itemgetter\n",
    "import pprint as pp\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "import nltk.data\n",
    "import time\n",
    "import proj_base\n",
    "from gensim import corpora, models\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "#Stemmer defined for stemming words\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "tokenizer2 = RegexpTokenizer(r'\\w+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1111, 13)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = proj_base.getStandardData()\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#creating the vocab of all the words\n",
    "\n",
    "def lintWord(w):\n",
    "    regex = re.compile(r'(,|\\(|\\)|!|:|$|\\.)')\n",
    "    w = re.sub(regex, '', w)\n",
    "    return w\n",
    "\n",
    "\n",
    "allWords = \"\"\n",
    "for r in data[\"Content\"]:\n",
    "    #add word to big content string\n",
    "    allWords += r + \" \"\n",
    "    \n",
    "#split the string at spaces, keep only unique\n",
    "words = set(allWords.split(\" \"))\n",
    "\n",
    "\n",
    "vocab = list(set([lintWord(w) for w in words if not w in stopwords.words(\"english\")]))\n",
    "\n",
    "#n eed to remove stopwords again because some of them may have had punctuation \n",
    "# at the end and didnt get caught the first time\n",
    "vocab = [w for w in vocab if not w in stopwords.words(\"english\") and len(w) > 2]\n",
    "\n",
    "#vocab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "seeds =  {\"Value\" : [\"value\", \"price\", \"quality\",\"worth\"],\n",
    "          \"Rooms\" : [\"room\", \"suite\", \"view\", \"bed\"],\n",
    "          \"Location\" : [\"location\", \"traffic\", \"minute\", \"restaurant\"],\n",
    "          \"Cleanliness\" : [\"clean\", \"dirty\", \"maintain\", \"smell\"],\n",
    "          \"Check in / front desk\": [\"stuff\", \"check\", \"help\", \"reservation\"],\n",
    "          \"Service\" : [\"service\", \"food\", \"breakfast\", \"buffet\"],\n",
    "          \"Business service\" : [\"business\", \"center\", \"computer\", \"internet\"]\n",
    "         }\n",
    "seedsBayes = copy.deepcopy(seeds)\n",
    "seedsLDA = copy.deepcopy(seeds)\n",
    "seedsCsq = copy.deepcopy(seeds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# alternate method to get keywords. LARA method based on Chi-square is basically same, but much better\n",
    "def aspectSegmentationBayes(reviews, seeds, freq_threshold = .5, prob_threshold = 0.2, words_per_iter = 4, iters = 3):\n",
    "\n",
    "    #break down reviews into sentences and break down each sentence into words using tokenizer and remove stopwords\n",
    "    # returns list where each item is the list of words in that sentence\n",
    "    sentence_words = []\n",
    "    for review in reviews:\n",
    "        review = review.decode('utf-8')\n",
    "        sentences = nltk.tokenize.sent_tokenize(review)\n",
    "        for sentence in sentences:\n",
    "            sentence_words.append([x.lower() for x in nltk.tokenize.word_tokenize(sentence) if x not in stopwords.words('english') and len(x) > 2])    \n",
    "    \n",
    "\n",
    "    # find Probability(sentence(S) has aspect(A) GIVEN S has word(W)) = count(S that have A and have W) / count(S that have W)\n",
    "\n",
    "    for i in range(iters):\n",
    "        \n",
    "        sents_with_word_asp = {}\n",
    "        sents_with_word = {}\n",
    "        sents_with_aspect = {}\n",
    "        prob_asp_given_word = {}\n",
    "\n",
    "        # calculates counts of (S that have W) and (S that have A and W)\n",
    "        for sentence in sentence_words:\n",
    "            for word in sentence:\n",
    "                sents_with_word[word] = sents_with_word.get(word,0) + 1\n",
    "                for aspect, aspect_words in seeds.items():\n",
    "                    for aspect_word in aspect_words:\n",
    "                        if aspect_word in sentence:\n",
    "                            sents_with_word_asp[(word,aspect)] = sents_with_word_asp.get((word, aspect), 0) + 1\n",
    "                            sents_with_aspect[aspect] = sents_with_aspect.get(aspect,0) + 1\n",
    "                            break\n",
    "\n",
    "        for (word, aspect), count in sents_with_word_asp.items():\n",
    "            #susceptible to low frequencies. hence freq_threshold\n",
    "            #freq_threshold ensures that count(S with  W) is atleast x% of count(S)\n",
    "            if sents_with_word[word] > (freq_threshold/100.0)*len(sentence_words):\n",
    "                prob_asp_given_word[(word,aspect)] = count/float(sents_with_word[word])\n",
    "\n",
    "        prob_asp_given_word_sorted = sorted(prob_asp_given_word.items(), key=itemgetter(1),reverse=True)\n",
    "        \n",
    "        for aspect, word_list in seeds.items():\n",
    "            count = 0\n",
    "            for item in prob_asp_given_word_sorted:\n",
    "                #item is of the form ((word,aspect),probability)\n",
    "                if item[0][1] == aspect:\n",
    "                    if item[0][0] not in word_list:\n",
    "                        if count <= words_per_iter:\n",
    "                            if item[1] >= prob_threshold:\n",
    "                                seeds[aspect].append(item[0][0])\n",
    "                                count += 1\n",
    "                            else:\n",
    "                                #because sorted, the others can't have higher probability\n",
    "                                break\n",
    "                        else:\n",
    "                            # because limiit of words per aspect in this iteration has been reached\n",
    "                            break\n",
    "\n",
    "    return seeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Business service': ['business',\n",
       "  'center',\n",
       "  'computer',\n",
       "  'internet',\n",
       "  u'access',\n",
       "  u'free',\n",
       "  u'monorail',\n",
       "  u'charge',\n",
       "  u'bus',\n",
       "  u'shuttle',\n",
       "  u'blocks',\n",
       "  u'easy',\n",
       "  u'provided',\n",
       "  u'waterfront',\n",
       "  u'airport',\n",
       "  u'pike',\n",
       "  u'shopping',\n",
       "  u'away'],\n",
       " 'Check in / front desk': ['stuff',\n",
       "  'check',\n",
       "  'help',\n",
       "  'reservation',\n",
       "  u'clerk',\n",
       "  u'early',\n",
       "  u'late',\n",
       "  u'arrived',\n",
       "  u'morning',\n",
       "  u'luggage',\n",
       "  u'called',\n",
       "  u'hours',\n",
       "  u'told',\n",
       "  u'checked',\n",
       "  u'desk',\n",
       "  u'said'],\n",
       " 'Cleanliness': ['clean',\n",
       "  'dirty',\n",
       "  'maintain',\n",
       "  'smell',\n",
       "  u'shared',\n",
       "  u'bathrooms',\n",
       "  u'comfortable',\n",
       "  u'spacious',\n",
       "  u'quiet',\n",
       "  u'beds',\n",
       "  u'large',\n",
       "  u'bed',\n",
       "  u'bathroom',\n",
       "  u'rooms',\n",
       "  u'size',\n",
       "  u'king',\n",
       "  u'huge',\n",
       "  u'bath',\n",
       "  u'small'],\n",
       " 'Location': ['location',\n",
       "  'traffic',\n",
       "  'minute',\n",
       "  'restaurant',\n",
       "  u'perfect',\n",
       "  u'excellent',\n",
       "  u'great',\n",
       "  u'convenient',\n",
       "  u'shopping',\n",
       "  u'value',\n",
       "  u'restaurants',\n",
       "  u'choice',\n",
       "  u'monorail',\n",
       "  u'pike',\n",
       "  u'market',\n",
       "  u'distance',\n",
       "  u'shops',\n",
       "  u'waterfront',\n",
       "  u'blocks'],\n",
       " 'Rooms': ['room',\n",
       "  'suite',\n",
       "  'view',\n",
       "  'bed',\n",
       "  u'king',\n",
       "  u'spacious',\n",
       "  u'floor',\n",
       "  u'size',\n",
       "  u'request',\n",
       "  u'large',\n",
       "  u'quiet',\n",
       "  u'upon',\n",
       "  u'corner',\n",
       "  u'huge',\n",
       "  u'beds',\n",
       "  u'comfortable',\n",
       "  u'window',\n",
       "  u'floors',\n",
       "  u'given'],\n",
       " 'Service': ['service',\n",
       "  'food',\n",
       "  'breakfast',\n",
       "  'buffet',\n",
       "  u'restaurant',\n",
       "  u'dinner',\n",
       "  u'coffee',\n",
       "  u'lounge',\n",
       "  u'shuttle',\n",
       "  u'eat',\n",
       "  u'concierge',\n",
       "  u'morning',\n",
       "  u'excellent',\n",
       "  u'hour',\n",
       "  u'places',\n",
       "  u'wine',\n",
       "  u'evening',\n",
       "  u'airport',\n",
       "  u'bar'],\n",
       " 'Value': ['value',\n",
       "  'price',\n",
       "  'quality',\n",
       "  'worth',\n",
       "  u'money',\n",
       "  u'paid',\n",
       "  u'per',\n",
       "  u'pay',\n",
       "  u'extra',\n",
       "  u'expensive',\n",
       "  u'charge',\n",
       "  u'parking',\n",
       "  u'lot']}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sds = aspectSegmentationBayes(data[\"Content\"], seedsBayes)\n",
    "sds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def aspectSegmentationLDA(reviews, aspects, vocab=[], threshold=0, iterationLimit=3):\n",
    "    #when we have the top chi-squared rated keywords, how many do we take\n",
    "    keywordsToTake = 3\n",
    "    \n",
    "    #bootstrap iterations\n",
    "    for i in range(0, iterationLimit):\n",
    "        \n",
    "        #print our current aspects\n",
    "        print(\"begin bootstrapping iteration, aspect keywords: \")\n",
    "        print(aspects)\n",
    "        print(\"\\n\\n\\n\")\n",
    "        labeledSentences = []\n",
    "        for r in reviews:\n",
    "            \n",
    "            #use the pickle tokenizer to split sentences\n",
    "                 \n",
    "            sentences = tokenizer.tokenize(r.decode('utf-8'))\n",
    "\n",
    "                \n",
    "            for s in sentences:\n",
    "                \n",
    "                theseAspects = collections.defaultdict(int)\n",
    "                maxAspect = (0, \"None\")\n",
    "                \n",
    "                #for each aspect count how many times one of those aspect words appears\n",
    "                for a in aspects:\n",
    "                    for word in aspects[a]:\n",
    "                        if \" \"+word+\" \" in s:\n",
    "                            theseAspects[a] += 1\n",
    "                \n",
    "                #find the max occuring aspect for each sentence, take multiple if ties\n",
    "                for a in theseAspects:\n",
    "                    if theseAspects[a] > maxAspect[0]:\n",
    "                        maxAspect = (theseAspects[a], a)\n",
    "                    if theseAspects[a] == maxAspect[0] and a not in maxAspect:\n",
    "                        #label it with multiple aspects\n",
    "                        maxAspect = maxAspect + (a, )\n",
    "                \n",
    "                #add the sentence with labels\n",
    "                labeledSentences.append((s, maxAspect[1:]))\n",
    "            \n",
    "        LDAForAspects = collections.defaultdict(list)\n",
    "        \n",
    "# obtain dictionary of tokenized sentences corresponding to each aspect\n",
    "        for a in aspects:\n",
    "            for s in labeledSentences:\n",
    "                sentenceText=s[0]\n",
    "                sentenceAspects=s[1]\n",
    "                \n",
    "                if a in sentenceAspects:\n",
    "                    sentenceTextTokens=tokenizer2.tokenize(sentenceText)\n",
    "                    sentenceTextTokensNS=[q for q in sentenceTextTokens if not q in stopwords.words(\"english\") and q>2]\n",
    "                    sentenceTextTokensNS_stemmed = [stemmer.stem(q) for q in sentenceTextTokensNS]\n",
    "                    LDAForAspects[a].extend(sentenceTextTokensNS)\n",
    "\n",
    "#Implement LDA to obtain topics\n",
    "        SentenceWords=[]\n",
    "        dictionary={}\n",
    "        corpus={}\n",
    "        for a in aspects:\n",
    "            #print(LDAForAspects[a])\n",
    "            SentenceWords.append(LDAForAspects[a])\n",
    "            dictionary[a]=corpora.Dictionary([LDAForAspects[a]])\n",
    "            corpus[a] = [dictionary[a].doc2bow([text]) for text in LDAForAspects[a]]\n",
    "            #print(a, corpus[a])\n",
    "            ldamodel = models.ldamodel.LdaModel(corpus[a], num_topics=1, id2word = dictionary[a], passes=20)\n",
    "            #print( a+\":\")\n",
    "            #print(ldamodel.print_topics(num_topics=1, num_words=5))\n",
    "            X=ldamodel.get_topic_terms(0,5)\n",
    "            i=0\n",
    "            while i<len(X):\n",
    "                temp=list(dictionary[a].token2id.keys())[list(dictionary[a].token2id.values()).index(X[i][0])]\n",
    "#     Checking for duplicates\n",
    "                if temp not in aspects[a]:  \n",
    "                    aspects[a].append(temp)\n",
    "                i=i+1\n",
    "            #print aspects[a]\n",
    "    return labeledSentences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "begin bootstrapping iteration, aspect keywords: \n",
      "{'Service': ['service', 'food', 'breakfast', 'buffet'], 'Business service': ['business', 'center', 'computer', 'internet'], 'Cleanliness': ['clean', 'dirty', 'maintain', 'smell'], 'Check in / front desk': ['stuff', 'check', 'help', 'reservation'], 'Value': ['value', 'price', 'quality', 'worth'], 'Rooms': ['room', 'suite', 'view', 'bed'], 'Location': ['location', 'traffic', 'minute', 'restaurant']}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "begin bootstrapping iteration, aspect keywords: \n",
      "{'Service': ['service', 'food', 'breakfast', 'buffet', u'room', u'hotel', u'great'], 'Business service': ['business', 'center', 'computer', 'internet', u'hotel', u'seattle'], 'Cleanliness': ['clean', 'dirty', 'maintain', 'smell', u'room', u'hotel', u'rooms', u'staff'], 'Check in / front desk': ['stuff', 'check', 'help', 'reservation', u'hotel', u'room', u'us'], 'Value': ['value', 'price', 'quality', 'worth', u'hotel', u'great', u'location'], 'Rooms': ['room', 'suite', 'view', 'bed', u'hotel', u'great'], 'Location': ['location', 'traffic', 'minute', 'restaurant', u'hotel', u'great', u'moore', u'room']}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "begin bootstrapping iteration, aspect keywords: \n",
      "{'Service': ['service', 'food', 'breakfast', 'buffet', u'room', u'hotel', u'great', u'place'], 'Business service': ['business', 'center', 'computer', 'internet', u'hotel', u'seattle', u'stay', u'place', u'downtown'], 'Cleanliness': ['clean', 'dirty', 'maintain', 'smell', u'room', u'hotel', u'rooms', u'staff'], 'Check in / front desk': ['stuff', 'check', 'help', 'reservation', u'hotel', u'room', u'us', u'one', u'would'], 'Value': ['value', 'price', 'quality', 'worth', u'hotel', u'great', u'location', u'seattle'], 'Rooms': ['room', 'suite', 'view', 'bed', u'hotel', u'great'], 'Location': ['location', 'traffic', 'minute', 'restaurant', u'hotel', u'great', u'moore', u'room']}\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Business service': ['business',\n",
       "  'center',\n",
       "  'computer',\n",
       "  'internet',\n",
       "  u'hotel',\n",
       "  u'seattle',\n",
       "  u'stay',\n",
       "  u'place',\n",
       "  u'downtown'],\n",
       " 'Check in / front desk': ['stuff',\n",
       "  'check',\n",
       "  'help',\n",
       "  'reservation',\n",
       "  u'hotel',\n",
       "  u'room',\n",
       "  u'us',\n",
       "  u'one',\n",
       "  u'would'],\n",
       " 'Cleanliness': ['clean',\n",
       "  'dirty',\n",
       "  'maintain',\n",
       "  'smell',\n",
       "  u'room',\n",
       "  u'hotel',\n",
       "  u'rooms',\n",
       "  u'staff'],\n",
       " 'Location': ['location',\n",
       "  'traffic',\n",
       "  'minute',\n",
       "  'restaurant',\n",
       "  u'hotel',\n",
       "  u'great',\n",
       "  u'moore',\n",
       "  u'room'],\n",
       " 'Rooms': ['room', 'suite', 'view', 'bed', u'hotel', u'great'],\n",
       " 'Service': ['service',\n",
       "  'food',\n",
       "  'breakfast',\n",
       "  'buffet',\n",
       "  u'room',\n",
       "  u'hotel',\n",
       "  u'great',\n",
       "  u'place'],\n",
       " 'Value': ['value',\n",
       "  'price',\n",
       "  'quality',\n",
       "  'worth',\n",
       "  u'hotel',\n",
       "  u'great',\n",
       "  u'location',\n",
       "  u'seattle']}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ldaAspects = aspectSegmentationLDA(data[\"Content\"], seedsLDA, vocab)\n",
    "seedsLDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def aspectSegmentationChiSquared(reviews, aspects, vocab=[], threshold=0, iterationLimit=3):\n",
    "    #when we have the top chi-squared rated keywords, how many do we take\n",
    "    keywordsToTake = 3\n",
    "    \n",
    "    #bootstrap iterations\n",
    "    for i in range(0, iterationLimit):\n",
    "        \n",
    "        #print our current aspects\n",
    "        print(\"begin bootstrapping iteration, aspect keywords: \")\n",
    "        pp.pprint(aspects)\n",
    "        print(\"\\n\\n\\n\")\n",
    "        labeledSentences = []\n",
    "        for r in reviews:\n",
    "            #use the pickle tokenizer to split sentences\n",
    "            sentences = tokenizer.tokenize(r)\n",
    "            \n",
    "            for s in sentences:\n",
    "                \n",
    "                theseAspects = collections.defaultdict(int)\n",
    "                maxAspect = (0, \"None\")\n",
    "                \n",
    "                #for each aspect count how many times one of those aspect words appears\n",
    "                for a in aspects:\n",
    "                    for word in aspects[a]:\n",
    "                        if \" \"+word+\" \" in s:\n",
    "                            theseAspects[a] += 1\n",
    "                \n",
    "                #find the max occuring aspect for each sentence, take multiple if ties\n",
    "                for a in theseAspects:\n",
    "                    if theseAspects[a] > maxAspect[0]:\n",
    "                        maxAspect = (theseAspects[a], a)\n",
    "                    if theseAspects[a] == maxAspect[0] and a not in maxAspect:\n",
    "                        #label it with multiple aspects\n",
    "                        maxAspect = maxAspect + (a, )\n",
    "                \n",
    "                #add the sentence with labels\n",
    "                labeledSentences.append((s, maxAspect[1:]))\n",
    "            \n",
    "        \n",
    "        chiSquaredForAspects = collections.defaultdict(list)\n",
    "        #calculate chi squared measure for each word in vocab\n",
    "        \"\"\"c1 is the number of times w occurs in sentences belonging\n",
    "            to aspect a_i, c2 is the number of times w occurs\n",
    "            in sentences not belonging to a_i, c3 is the number of sentences\n",
    "            of aspect a_i that do not contain w, c4 is the number\n",
    "            of sentences that neither belong to aspect a_i, nor contain\n",
    "            word w, and C is the total number of word occurrences\"\"\"\n",
    "        for w in vocab:\n",
    "            for a in aspects:\n",
    "                c = 0\n",
    "                c_1 = 0\n",
    "                c_2 = 0 \n",
    "                c_3 = 0\n",
    "                c_4 = 0\n",
    "\n",
    "                for s in labeledSentences:\n",
    "                    sentenceText = s[0]\n",
    "                    sentenceAspects = s[1]\n",
    "\n",
    "                    if \" \"+w+\" \" in sentenceText and a in sentenceAspects:\n",
    "                        c_1 += 1\n",
    "                    elif \" \"+w+\" \" in sentenceText and a not in sentenceAspects:\n",
    "                        c_2 += 1\n",
    "                    elif a in sentenceAspects and w not in sentenceText:\n",
    "                        c_3 += 1\n",
    "                    else:\n",
    "                        c_4 += 1\n",
    "\n",
    "                numer = ((1.0 * c_1 * c_4 - 1.0 * c_2 * c_3)**2)\n",
    "                denom = (1.0*(c_1 + c_3) * 1.0 * (c_2 + c_4) * 1.0 * (c_1 + c_2) * 1.0 * (c_3 + c_4))\n",
    "                #unreasonable use of 1.0's here to be safe\n",
    "                if denom != 0:\n",
    "                    csq = numer / denom\n",
    "                    chiSquaredForAspects[a].append((w, csq))\n",
    "\n",
    "\n",
    "        #have the chi squared aspects for each word in vocab, add top kewordsToTake for each aspect\n",
    "        for a in chiSquaredForAspects:\n",
    "            #make sure were not taking words we already have\n",
    "            noDupes = [tup for tup in chiSquaredForAspects[a] if tup[0] not in aspects[a]]\n",
    "            chiSquaredForAspects[a] = sorted(noDupes, key=itemgetter(1), reverse=True)[0:keywordsToTake]\n",
    "            for t in chiSquaredForAspects[a]:\n",
    "                if t[0] not in aspects[a] and t[0] != '':\n",
    "                    aspects[a].append(t[0])\n",
    "            \n",
    "            \n",
    "        \n",
    "            #split into sentences\n",
    "        \n",
    "        #loop through again\n",
    "    return seeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "begin bootstrapping iteration, aspect keywords: \n",
      "{'Business service': ['business', 'center', 'computer', 'internet'],\n",
      " 'Check in / front desk': ['stuff', 'check', 'help', 'reservation'],\n",
      " 'Cleanliness': ['clean', 'dirty', 'maintain', 'smell'],\n",
      " 'Location': ['location', 'traffic', 'minute', 'restaurant'],\n",
      " 'Rooms': ['room', 'suite', 'view', 'bed'],\n",
      " 'Service': ['service', 'food', 'breakfast', 'buffet'],\n",
      " 'Value': ['value', 'price', 'quality', 'worth']}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "begin bootstrapping iteration, aspect keywords: \n",
      "{'Business service': ['business',\n",
      "                      'center',\n",
      "                      'computer',\n",
      "                      'internet',\n",
      "                      'access',\n",
      "                      'westlake',\n",
      "                      'wireless'],\n",
      " 'Check in / front desk': ['stuff',\n",
      "                           'check',\n",
      "                           'help',\n",
      "                           'reservation',\n",
      "                           'clerk',\n",
      "                           'told',\n",
      "                           'later'],\n",
      " 'Cleanliness': ['clean',\n",
      "                 'dirty',\n",
      "                 'maintain',\n",
      "                 'smell',\n",
      "                 'room',\n",
      "                 'garbage',\n",
      "                 'rid'],\n",
      " 'Location': ['location',\n",
      "              'traffic',\n",
      "              'minute',\n",
      "              'restaurant',\n",
      "              'great',\n",
      "              'pike',\n",
      "              'central'],\n",
      " 'Rooms': ['room', 'suite', 'view', 'bed', 'floor', 'king', 'clean'],\n",
      " 'Service': ['service',\n",
      "             'food',\n",
      "             'breakfast',\n",
      "             'buffet',\n",
      "             'continental',\n",
      "             'restaurant',\n",
      "             'customer'],\n",
      " 'Value': ['value', 'price', 'quality', 'worth', 'money', 'location', 'beat']}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "begin bootstrapping iteration, aspect keywords: \n",
      "{'Business service': ['business',\n",
      "                      'center',\n",
      "                      'computer',\n",
      "                      'internet',\n",
      "                      'access',\n",
      "                      'westlake',\n",
      "                      'wireless',\n",
      "                      'free',\n",
      "                      'convention',\n",
      "                      'terminal'],\n",
      " 'Check in / front desk': ['stuff',\n",
      "                           'check',\n",
      "                           'help',\n",
      "                           'reservation',\n",
      "                           'clerk',\n",
      "                           'told',\n",
      "                           'later',\n",
      "                           'desk',\n",
      "                           'asked',\n",
      "                           'front'],\n",
      " 'Cleanliness': ['clean',\n",
      "                 'dirty',\n",
      "                 'maintain',\n",
      "                 'smell',\n",
      "                 'room',\n",
      "                 'garbage',\n",
      "                 'rid',\n",
      "                 'shared',\n",
      "                 'small',\n",
      "                 'spacious'],\n",
      " 'Location': ['location',\n",
      "              'traffic',\n",
      "              'minute',\n",
      "              'restaurant',\n",
      "              'great',\n",
      "              'pike',\n",
      "              'central',\n",
      "              'place',\n",
      "              'market',\n",
      "              'walk'],\n",
      " 'Rooms': ['room',\n",
      "           'suite',\n",
      "           'view',\n",
      "           'bed',\n",
      "           'floor',\n",
      "           'king',\n",
      "           'clean',\n",
      "           'comfortable',\n",
      "           'large',\n",
      "           'corner'],\n",
      " 'Service': ['service',\n",
      "             'food',\n",
      "             'breakfast',\n",
      "             'buffet',\n",
      "             'continental',\n",
      "             'restaurant',\n",
      "             'customer',\n",
      "             'ordered',\n",
      "             'ate',\n",
      "             'fast'],\n",
      " 'Value': ['value',\n",
      "           'price',\n",
      "           'quality',\n",
      "           'worth',\n",
      "           'money',\n",
      "           'location',\n",
      "           'beat',\n",
      "           'good',\n",
      "           'perfect',\n",
      "           'downtown']}\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "csqSeeds = aspectSegmentationChiSquared(data[\"Content\"], seedsCsq, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "seedsCsq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
