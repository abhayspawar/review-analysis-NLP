{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1759\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "import collections\n",
    "from operator import itemgetter\n",
    "import pprint as pp\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "import nltk.data\n",
    "import time\n",
    "import proj_base\n",
    "#data from http://times.cs.uiuc.edu/~wang296/Data/\n",
    "files = os.listdir('./Review_Texts')\n",
    "#print(os.listdir('./Review_Texts'))\n",
    "print(len(files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'hotel_100504.dat'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-7c8fa7e38f82>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mproj_base\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetStandardData\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/conraddepeuter/Documents/columbia/nlp/project/proj_base.py\u001b[0m in \u001b[0;36mgetStandardData\u001b[0;34m(numFiles)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mfile\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mnumFiles\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maddFileToData\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/conraddepeuter/Documents/columbia/nlp/project/proj_base.py\u001b[0m in \u001b[0;36maddFileToData\u001b[0;34m(filename, data)\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mintColumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'No. Reader'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'No. Helpful'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Cleanliness'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'Check in / front desk'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Value'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Overall'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Service'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Business service'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Rooms'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Location'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mcharacterThreshold\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m60\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcontent_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0mcontent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontent_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'hotel_100504.dat'"
     ]
    }
   ],
   "source": [
    "data = proj_base.getStandardData()\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(data.shape)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#mean of int columns\n",
    "data.mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data[\"Content\"][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#creating the vocab of all the words\n",
    "\n",
    "def lintWord(w):\n",
    "    regex = re.compile(r'(,|\\(|\\)|!|:|$|\\.)')\n",
    "    w = re.sub(regex, '', w)\n",
    "    return w\n",
    "\n",
    "\n",
    "allWords = \"\"\n",
    "for r in data[\"Content\"]:\n",
    "    #add word to big content string\n",
    "    allWords += r + \" \"\n",
    "    \n",
    "#split the string at spaces, keep only unique\n",
    "words = set(allWords.split(\" \"))\n",
    "\n",
    "\n",
    "vocab = list(set([lintWord(w) for w in words if not w in stopwords.words(\"english\")]))\n",
    "\n",
    "#n eed to remove stopwords again because some of them may have had punctuation \n",
    "# at the end and didnt get caught the first time\n",
    "vocab = [w for w in vocab if not w in stopwords.words(\"english\") and len(w) > 2]\n",
    "\n",
    "#vocab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(len(vocab))\n",
    "print(len(set(vocab)))\n",
    "print(\"i\" in vocab)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"Algorithm: Aspect Segmentation Algorithm\n",
    "Input: A collection of reviews {푑1, 푑2, . . . , 푑∣퐷∣)}, set of\n",
    "aspect keywords {푇1, 푇2, . . . , 푇푘}, vocabulary V, selection\n",
    "threshold p and iteration step limit I.\n",
    "Output: Reviews split into sentences with aspect assignments.\n",
    "Step 0: Split all reviews into sentences, 푋 =\n",
    "{푥1, 푥2, . . . , 푥푀};\n",
    "Step 1: Match the aspect keywords in each sentence\n",
    "of X and record the matching hits for each aspect i in\n",
    "퐶표푢푛푡(푖);\n",
    "Step 2: Assign the sentence an aspect label by 푎푖 =\n",
    "푎푟푔푚푎푥푖 퐶표푢푛푡(푖). If there is a tie, assign the sentence\n",
    "with multiple aspects.\n",
    "Step 3: Calculate chi^2 measure of each word (in V);\n",
    "Step 4: Rank the words under each aspect with respect\n",
    "to their chi^2value and join the top p words for each aspect\n",
    "into their corresponding aspect keyword list 푇푖;\n",
    "Step 5: If the aspect keyword list is unchanged or iteration\n",
    "exceeds I, go to Step 6, else go to Step 1;\n",
    "Step 6: Output the annotated sentences with aspect\n",
    "assignments.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#tokenizer to split sentences\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# initial seeds from LARA paper\n",
    "seeds = {\"Value\" : [\"value\", \"price\", \"quality\",\"worth\"],\n",
    "         \"Rooms\" : [\"room\", \"suite\", \"view\", \"bed\"],\n",
    "         \"Location\" : [\"location\", \"traffic\", \"minute\", \"restaurant\"],\n",
    "         \"Cleanliness\" : [\"clean\", \"dirty\", \"maintain\", \"smell\"],\n",
    "         \"Check in / front desk\": [\"stuff\", \"check\", \"help\", \"reservation\"],\n",
    "         \"Service\" : [\"service\", \"food\", \"breakfast\", \"buffet\"],\n",
    "         \"Business service\" : [\"business\", \"center\", \"computer\", \"internet\"]\n",
    "        }\n",
    "\n",
    "\n",
    "def aspectSegmentation(reviews, aspects, vocab=[], threshold=0, iterationLimit=3):\n",
    "    #when we have the top chi-squared rated keywords, how many do we take\n",
    "    keywordsToTake = 3\n",
    "    \n",
    "    #bootstrap iterations\n",
    "    for i in range(0, iterationLimit):\n",
    "        \n",
    "        #print our current aspects\n",
    "        print(\"begin bootstrapping iteration, aspect keywords: \")\n",
    "        pp.pprint(aspects)\n",
    "        print(\"\\n\\n\\n\")\n",
    "        labeledSentences = []\n",
    "        for r in reviews:\n",
    "            #use the pickle tokenizer to split sentences\n",
    "            sentences = tokenizer.tokenize(r)\n",
    "            \n",
    "            for s in sentences:\n",
    "                \n",
    "                theseAspects = collections.defaultdict(int)\n",
    "                maxAspect = (0, \"None\")\n",
    "                \n",
    "                #for each aspect count how many times one of those aspect words appears\n",
    "                for a in aspects:\n",
    "                    for word in aspects[a]:\n",
    "                        if \" \"+word+\" \" in s:\n",
    "                            theseAspects[a] += 1\n",
    "                \n",
    "                #find the max occuring aspect for each sentence, take multiple if ties\n",
    "                for a in theseAspects:\n",
    "                    if theseAspects[a] > maxAspect[0]:\n",
    "                        maxAspect = (theseAspects[a], a)\n",
    "                    if theseAspects[a] == maxAspect[0] and a not in maxAspect:\n",
    "                        #label it with multiple aspects\n",
    "                        maxAspect = maxAspect + (a, )\n",
    "                \n",
    "                #add the sentence with labels\n",
    "                labeledSentences.append((s, maxAspect[1:]))\n",
    "            \n",
    "        \n",
    "        chiSquaredForAspects = collections.defaultdict(list)\n",
    "        #calculate chi squared measure for each word in vocab\n",
    "        \"\"\"c1 is the number of times w occurs in sentences belonging\n",
    "            to aspect a_i, c2 is the number of times w occurs\n",
    "            in sentences not belonging to a_i, c3 is the number of sentences\n",
    "            of aspect a_i that do not contain w, c4 is the number\n",
    "            of sentences that neither belong to aspect a_i, nor contain\n",
    "            word w, and C is the total number of word occurrences\"\"\"\n",
    "        for w in vocab:\n",
    "            for a in aspects:\n",
    "                c = 0\n",
    "                c_1 = 0\n",
    "                c_2 = 0 \n",
    "                c_3 = 0\n",
    "                c_4 = 0\n",
    "\n",
    "                for s in labeledSentences:\n",
    "                    sentenceText = s[0]\n",
    "                    sentenceAspects = s[1]\n",
    "\n",
    "                    if \" \"+w+\" \" in sentenceText and a in sentenceAspects:\n",
    "                        c_1 += 1\n",
    "                    elif \" \"+w+\" \" in sentenceText and a not in sentenceAspects:\n",
    "                        c_2 += 1\n",
    "                    elif a in sentenceAspects and w not in sentenceText:\n",
    "                        c_3 += 1\n",
    "                    else:\n",
    "                        c_4 += 1\n",
    "\n",
    "                numer = ((1.0 * c_1 * c_4 - 1.0 * c_2 * c_3)**2)\n",
    "                denom = (1.0*(c_1 + c_3) * 1.0 * (c_2 + c_4) * 1.0 * (c_1 + c_2) * 1.0 * (c_3 + c_4))\n",
    "                #unreasonable use of 1.0's here to be safe\n",
    "                if denom != 0:\n",
    "                    csq = numer / denom\n",
    "                    chiSquaredForAspects[a].append((w, csq))\n",
    "\n",
    "\n",
    "        #have the chi squared aspects for each word in vocab, add top kewordsToTake for each aspect\n",
    "        for a in chiSquaredForAspects:\n",
    "            #make sure were not taking words we already have\n",
    "            noDupes = [tup for tup in chiSquaredForAspects[a] if tup[0] not in aspects[a]]\n",
    "            chiSquaredForAspects[a] = sorted(noDupes, key=itemgetter(1), reverse=True)[0:keywordsToTake]\n",
    "            for t in chiSquaredForAspects[a]:\n",
    "                if t[0] not in aspects[a] and t[0] != '':\n",
    "                    aspects[a].append(t[0])\n",
    "            \n",
    "            \n",
    "        \n",
    "            #split into sentences\n",
    "        \n",
    "        #loop through again\n",
    "    return labeledSentences\n",
    "    #return labeledSentences\n",
    "\n",
    "start = time.time()\n",
    "sentencesWLabels = aspectSegmentation(data[\"Content\"], seeds, vocab)\n",
    "end = time.time()\n",
    "\n",
    "print(\"done, time taken:\", end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "numToShow = 20\n",
    "count = 0\n",
    "for s in sentencesWLabels:\n",
    "    if 'None' not in s[1]:\n",
    "        print(s)\n",
    "        print(\"\\n\")\n",
    "        count += 1\n",
    "    if count > numToShow:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#seeds will have changed, whats differet\n",
    "seeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def seedInReview(review, seeds):\n",
    "    for s in seeds:\n",
    "        #print(s)\n",
    "        if s in review:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "\n",
    "reviewsWithSeeds = {}\n",
    "for s in seeds:\n",
    "    reviewsWithSeeds[s] = sum(data[\"Content\"].apply(seedInReview, args = [seeds[s]]))\n",
    "reviewsWithSeeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# from gensim.models import word2vec\n",
    "# print(\"training\")\n",
    "# model = word2vec.Word2Vec(sentences, workers=num_workers, size=num_features)\n",
    "\n",
    "\n",
    "#LDA analysis"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
