{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once deleted, variables cannot be recovered. Proceed (y/[n])? y\n"
     ]
    }
   ],
   "source": [
    "%reset\n",
    "import nltk.data\n",
    "import nltk.tokenize\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "\n",
    "class Splitter(object):\n",
    "    def __init__(self):\n",
    "        self.nltk_splitter = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "        self.nltk_tokenizer = nltk.tokenize.TreebankWordTokenizer()\n",
    "\n",
    "    def split(self, text):\n",
    "        \"\"\"\n",
    "        input format: a paragraph of text\n",
    "        output format: a list of lists of words.\n",
    "            e.g.: [['this', 'is', 'a', 'sentence'], ['this', 'is', 'another', 'one']]\n",
    "        \"\"\"\n",
    "        sentences = self.nltk_splitter.tokenize(text)\n",
    "        tokenized_sentences = [self.nltk_tokenizer.tokenize(sent) for sent in sentences]\n",
    "        return tokenized_sentences\n",
    "\n",
    "\n",
    "class POSTagger(object):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "        \n",
    "    def pos_tag(self, sentences):\n",
    "        \"\"\"\n",
    "        input format: list of lists of words\n",
    "            e.g.: [['this', 'is', 'a', 'sentence'], ['this', 'is', 'another', 'one']]\n",
    "        output format: list of lists of tagged tokens. Each tagged tokens has a\n",
    "        form, a lemma, and a list of tags\n",
    "            e.g: [[('this', 'this', ['DT']), ('is', 'be', ['VB']), ('a', 'a', ['DT']), ('sentence', 'sentence', ['NN'])],\n",
    "                    [('this', 'this', ['DT']), ('is', 'be', ['VB']), ('another', 'another', ['DT']), ('one', 'one', ['CARD'])]]\n",
    "        \"\"\"\n",
    "\n",
    "        pos = [nltk.pos_tag(sentence) for sentence in sentences]\n",
    "        #adapt format\n",
    "        pos = [[(word, word, [postag]) for (word, postag) in sentence] for sentence in pos]\n",
    "        return pos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#from yaml import load, dump\n",
    "import yaml\n",
    "class DictionaryTagger(object):\n",
    "    def __init__(self, dictionary_paths):\n",
    "        files = [open(path, 'r') for path in dictionary_paths]\n",
    "            \n",
    "        dictionaries = [yaml.load(dict_file) for dict_file in files]\n",
    "        #dictionaries = [dict_positive,dict_negative]\n",
    "\n",
    "        map(lambda x: x.close(), files)\n",
    "        self.dictionary = {}\n",
    "        self.max_key_size = 0\n",
    "        for curr_dict in dictionaries:\n",
    "            for key in curr_dict:\n",
    "                if key in self.dictionary:\n",
    "                    self.dictionary[key].extend(curr_dict[key])\n",
    "                else:\n",
    "                    self.dictionary[key] = curr_dict[key]\n",
    "                    self.max_key_size = max(self.max_key_size, len(key))\n",
    "\n",
    "    def tag(self, postagged_sentences):\n",
    "        return [self.tag_sentence(sentence) for sentence in postagged_sentences]\n",
    "\n",
    "    def tag_sentence(self, sentence, tag_with_lemmas=False):\n",
    "        \"\"\"\n",
    "        the result is only one tagging of all the possible ones.\n",
    "        The resulting tagging is determined by these two priority rules:\n",
    "            - longest matches have higher priority\n",
    "            - search is made from left to right\n",
    "        \"\"\"\n",
    "        tag_sentence = []\n",
    "        N = len(sentence)\n",
    "        if self.max_key_size == 0:\n",
    "            self.max_key_size = N\n",
    "        i = 0\n",
    "        while (i < N):\n",
    "            j = min(i + self.max_key_size, N) #avoid overflow\n",
    "            tagged = False\n",
    "            while (j > i):\n",
    "                expression_form = ' '.join([word[0] for word in sentence[i:j]]).lower()\n",
    "                expression_lemma = ' '.join([word[1] for word in sentence[i:j]]).lower()\n",
    "                if tag_with_lemmas:\n",
    "                    literal = expression_lemma\n",
    "                else:\n",
    "                    literal = expression_form\n",
    "                if literal in self.dictionary:\n",
    "                    #self.logger.debug(\"found: %s\" % literal)\n",
    "                    is_single_token = j - i == 1\n",
    "                    original_position = i\n",
    "                    i = j\n",
    "                    taggings = [tag for tag in self.dictionary[literal]]\n",
    "                    tagged_expression = (expression_form, expression_lemma, taggings)\n",
    "                    if is_single_token: #if the tagged literal is a single token, conserve its previous taggings:\n",
    "                        original_token_tagging = sentence[original_position][2]\n",
    "                        tagged_expression[2].extend(original_token_tagging)\n",
    "                    tag_sentence.append(tagged_expression)\n",
    "                    tagged = True\n",
    "                else:\n",
    "                    j = j - 1\n",
    "            if not tagged:\n",
    "                tag_sentence.append(sentence[i])\n",
    "                i += 1\n",
    "        return tag_sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def value_of(sentiment):\n",
    "    if sentiment == 'positive': return 1\n",
    "    if sentiment == 'negative': return -1\n",
    "    return 0\n",
    "\n",
    "def value_of_pos(sentiment):\n",
    "    if sentiment == 'positive': return 1\n",
    "    return 0\n",
    "\n",
    "def value_of_neg(sentiment):\n",
    "    if sentiment == 'negative': return -1\n",
    "    return 0\n",
    "    \n",
    "def sentiment_score_basic_pos(sentences):\n",
    "    sm = 0\n",
    "    for sentence in sentences:\n",
    "        for token in sentence:\n",
    "            for tag in token[2]:\n",
    "                sm += value_of_pos(tag)\n",
    "    return sm\n",
    "\n",
    "def sentiment_score_basic_neg(sentences):    \n",
    "    sm = 0\n",
    "    for sentence in sentences:\n",
    "        for token in sentence:\n",
    "            for tag in token[2]:\n",
    "                sm += value_of_neg(tag)\n",
    "    return sm\n",
    "\n",
    "def sentiment_score_basic(sentences):    \n",
    "    sm = 0\n",
    "    for sentence in sentences:\n",
    "        for token in sentence:\n",
    "            for tag in token[2]:\n",
    "                sm += value_of(tag)\n",
    "    return sm\n",
    "\n",
    "def sentence_score(sentence_tokens, previous_token, acum_score):  \n",
    "    #$print(sentence_tokens)\n",
    "    previous_token = None\n",
    "    for current_token in sentence_tokens:\n",
    "        tags = current_token[2]\n",
    "        token_score = sum([value_of(tag) for tag in tags])\n",
    "        if previous_token is not None:\n",
    "            previous_tags = previous_token[2]\n",
    "            if 'inc' in previous_tags:\n",
    "                token_score *= 2.0\n",
    "            elif 'dec' in previous_tags:\n",
    "                token_score /= 2.0\n",
    "            elif 'inv' in previous_tags:\n",
    "                token_score *= -1.0\n",
    "        \n",
    "        \n",
    "        acum_score += token_score\n",
    "        previous_token = current_token\n",
    "    \n",
    "    return acum_score\n",
    "    \n",
    "def sentence_score_pos(sentence_tokens, previous_token, acum_score):    \n",
    "    previous_token = None\n",
    "    for current_token in sentence_tokens:\n",
    "        tags = current_token[2]\n",
    "        token_score = sum([value_of_pos(tag) for tag in tags])\n",
    "        if previous_token is not None:\n",
    "            previous_tags = previous_token[2]\n",
    "            if 'inc' in previous_tags:\n",
    "                token_score *= 2.0\n",
    "            elif 'dec' in previous_tags:\n",
    "                token_score /= 2.0\n",
    "            elif 'inv' in previous_tags:\n",
    "                token_score *= -1.0\n",
    "        \n",
    "        \n",
    "        acum_score += token_score\n",
    "        previous_token = current_token\n",
    "    \n",
    "    return acum_score\n",
    "\n",
    "def sentence_score_neg(sentence_tokens, previous_token, acum_score):    \n",
    "    previous_token = None\n",
    "    for current_token in sentence_tokens:\n",
    "        tags = current_token[2]\n",
    "        token_score = sum([value_of_neg(tag) for tag in tags])\n",
    "        if previous_token is not None:\n",
    "            previous_tags = previous_token[2]\n",
    "            if 'inc' in previous_tags:\n",
    "                token_score *= 2.0\n",
    "            elif 'dec' in previous_tags:\n",
    "                token_score /= 2.0\n",
    "            elif 'inv' in previous_tags:\n",
    "                token_score *= -1.0\n",
    "        \n",
    "        \n",
    "        acum_score += token_score\n",
    "        previous_token = current_token\n",
    "    \n",
    "    return acum_score\n",
    "\n",
    "def sentiment_score(review):\n",
    "    sm = 0\n",
    "    for sentence in review:\n",
    "        sm += sentence_score(sentence, None, 0.0)\n",
    "        \n",
    "    return sm\n",
    "\n",
    "def sentiment_score_pos(review):\n",
    "    sm = 0\n",
    "    for sentence in review:\n",
    "        sm += sentence_score_pos(sentence, None, 0.0)\n",
    "        \n",
    "    return sm\n",
    "\n",
    "def sentiment_score_neg(review):\n",
    "    sm = 0\n",
    "    for sentence in review:\n",
    "        sm += sentence_score_neg(sentence, None, 0.0)\n",
    "        \n",
    "    return sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import proj_base\n",
    "\n",
    "\n",
    "\n",
    "#TO CHANGE ASPECT uncomment this\n",
    "#proj_base.aspect = \"Location\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rooms</th>\n",
       "      <th>Date</th>\n",
       "      <th>Location</th>\n",
       "      <th>Service</th>\n",
       "      <th>Business service</th>\n",
       "      <th>Author</th>\n",
       "      <th>Check in / front desk</th>\n",
       "      <th>No. Helpful</th>\n",
       "      <th>Cleanliness</th>\n",
       "      <th>Content</th>\n",
       "      <th>Value</th>\n",
       "      <th>No. Reader</th>\n",
       "      <th>Overall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.0</td>\n",
       "      <td>Dec 23, 2008\\r</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>selizabethm\\r</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>wonderful time- even with the snow! what a gre...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.0</td>\n",
       "      <td>Nov 13, 2008\\r</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IndieLady\\r</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>lovely hotel, unique decor, friendly front des...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.0</td>\n",
       "      <td>Nov 11, 2008\\r</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>Hilobb\\r</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>nice hotel, expensive parking we got a good de...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.0</td>\n",
       "      <td>Nov 4, 2008\\r</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5</td>\n",
       "      <td>Chianti_girl24\\r</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>fabulous hotel location and service are great....</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Oct 18, 2008\\r</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>hothearted\\r</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>loved the monaco! staff was amazing, with a sm...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Rooms            Date  Location  Service Business service  \\\n",
       "0    5.0  Dec 23, 2008\\r       5.0      5.0              NaN   \n",
       "1    4.0  Nov 13, 2008\\r       5.0      5.0              NaN   \n",
       "2    4.0  Nov 11, 2008\\r       3.0      NaN                4   \n",
       "3    5.0   Nov 4, 2008\\r       5.0      5.0                5   \n",
       "4    NaN  Oct 18, 2008\\r       NaN      NaN              NaN   \n",
       "\n",
       "             Author  Check in / front desk No. Helpful  Cleanliness  \\\n",
       "0     selizabethm\\r                    5.0         NaN          5.0   \n",
       "1       IndieLady\\r                    5.0         NaN          4.0   \n",
       "2          Hilobb\\r                    5.0         NaN          4.0   \n",
       "3  Chianti_girl24\\r                    5.0         NaN          5.0   \n",
       "4      hothearted\\r                    NaN           2          NaN   \n",
       "\n",
       "                                             Content  Value No. Reader  \\\n",
       "0  wonderful time- even with the snow! what a gre...    4.0        NaN   \n",
       "1  lovely hotel, unique decor, friendly front des...    5.0        NaN   \n",
       "2  nice hotel, expensive parking we got a good de...    4.0        NaN   \n",
       "3  fabulous hotel location and service are great....    5.0        NaN   \n",
       "4  loved the monaco! staff was amazing, with a sm...    NaN          2   \n",
       "\n",
       "   Overall  \n",
       "0      5.0  \n",
       "1      4.0  \n",
       "2      4.0  \n",
       "3      5.0  \n",
       "4      5.0  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#getting the standard data if this takes too long drop it to 10 (but restart the kernel before as aspectSegmentation probably ran)\n",
    "\n",
    "data = proj_base.getStandardData(numFiles=20)\n",
    "proj_base.aspectSegmentationBayes(data[\"Content\"])\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "td = trainingData[0:3].apply(addSentimentScores, axis = 1)\n",
    "td"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(data.shape)\n",
    "trainingData = proj_base.getTrainingData(data)\n",
    "print(trainingData.shape)\n",
    "trainingData.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dicttagger = DictionaryTagger(['dicts/f_negative.yml', 'dicts/f_positive.yml','dicts/f_inc.yml','dicts/f_dec.yml','dicts/f_inv.yml'])\n",
    "splitter = Splitter()\n",
    "postagger = POSTagger()\n",
    "wordTokenizer = nltk.tokenize.TreebankWordTokenizer()\n",
    "\n",
    "def addSentimentScores(review):\n",
    "    \n",
    "    tokenizedSentences = [wordTokenizer.tokenize(sent) for sent in  review[\"aspectSentences\"]]\n",
    "    pos_tagged_sentences = postagger.pos_tag(tokenizedSentences)\n",
    "    dict_tagged_sentences = dicttagger.tag(pos_tagged_sentences)\n",
    "    \n",
    "    #review['sentiment']=dict_tagged_sentences\n",
    "    review['score'] = sentiment_score(dict_tagged_sentences)\n",
    "    review[\"score_basic_pos\"] = sentiment_score_basic_pos(dict_tagged_sentences)\n",
    "    review[\"score_basic_neg\"] = sentiment_score_basic_neg(dict_tagged_sentences)\n",
    "    review[\"score_basic\"] = sentiment_score_basic(dict_tagged_sentences)\n",
    "    review[\"score_pos\"] = sentiment_score_pos(dict_tagged_sentences)\n",
    "    review[\"score_neg\"] = sentiment_score_neg(dict_tagged_sentences)\n",
    "    return review\n",
    "\n",
    "#trainingData = trainingData.apply(addSentimentScores, axis = 1)\n",
    "\n",
    "trainingData.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#data['Overall'][data['Overall']!=0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#'score','score_basic_pos','score_basic_neg',\n",
    "#        'score_basic','score_pos','score_neg',\n",
    "\n",
    "score=data['score'][data['Overall']!=0].reshape(-1,1)\n",
    "score_basic_pos=data['score_basic_pos'][data['Overall']!=0].reshape(-1,1)\n",
    "score_basic_neg=data['score_basic_neg'][data['Overall']!=0].reshape(-1,1)\n",
    "score_basic=data['score_basic'][data['Overall']!=0].reshape(-1,1)\n",
    "score_pos=data['score_pos'][data['Overall']!=0].reshape(-1,1)\n",
    "score_neg=data['score_neg'][data['Overall']!=0].reshape(-1,1)\n",
    "\n",
    "X=[]\n",
    "for i in range(0,len(score)):\n",
    "    #X.extend([[score[i][0],score_basic_pos[i][0],score_basic_neg[i][0],score_basic[i][0],score_pos[i][0],score_neg[i][0]]])\n",
    "    X.extend([[score[i][0]]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#'score','score_basic_pos','score_basic_neg',\n",
    "#        'score_basic','score_pos','score_neg',\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.svm import SVC\n",
    "clf = SVC()\n",
    "#clf.fit(X[0:1000],data['Overall'][0:1000])\n",
    "clf.fit(X,data['Overall'][data['Overall']!=0]) \n",
    "\n",
    "#score(X, y, sample_weight=None)\n",
    "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
    "    decision_function_shape=None, degree=3, gamma='auto', kernel='rbf',\n",
    "    max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
    "    tol=0.001, verbose=False)\n",
    "pred=clf.predict(X)\n",
    "#score(data['score'].reshape(-1, 1), data['Overall'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sum=float(0)\n",
    "error=float(0)\n",
    "#actual=data['Overall'].reshape(-1,1)[1001:len(X)]\n",
    "actual=data['Overall'][data['Overall']!=0].reshape(-1,1)\n",
    "\n",
    "#print len(pred)\n",
    "#print len(actual)\n",
    "for i in range(0,len(pred)):\n",
    "    if pred[i]==actual[i][0]:\n",
    "        sum+=1\n",
    "    error_curr=abs(actual[i][0]-pred[i])\n",
    "    error+=error_curr\n",
    "accuracy=sum/len(pred)\n",
    "mean_error=error/len(pred)\n",
    "\n",
    "print accuracy\n",
    "print mean_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#code to create file in desired format\n",
    "\n",
    "file=open('C:/Users/Abhay Pawar/Documents/GitHub/data/opinion-lexicon-English/positive-words.txt')\n",
    "lines=file.readlines()\n",
    "dict_positive={}\n",
    "for line in lines:\n",
    "    dict_positive[line.strip()]=['positive']\n",
    "with open('dicts/f_positive.yml', 'w') as outfile:\n",
    "    yaml.dump(dict_positive, outfile, default_flow_style=True)\n",
    "    \n",
    "file=open('C:/Users/Abhay Pawar/Documents/GitHub/data/opinion-lexicon-English/negative-words.txt')\n",
    "lines=file.readlines()\n",
    "dict_negative={}\n",
    "for line in lines:\n",
    "    dict_negative[line.strip()]=['negative']\n",
    "with open('dicts/f_negative.yml', 'w') as outfile:\n",
    "    yaml.dump(dict_negative, outfile, default_flow_style=True)\n",
    "\n",
    "dict_inc={}\n",
    "dict_inc['too']= ['inc']\n",
    "dict_inc['very']= ['inc']\n",
    "dict_inc['sorely']= ['inc']\n",
    "dict_inc['extremely']= ['inc']\n",
    "dict_inc['really']= ['inc']\n",
    "with open('dicts/f_dec.yml', 'w') as outfile:\n",
    "    yaml.dump(dict_inc, outfile, default_flow_style=True)\n",
    "\n",
    "dict_dec={}\n",
    "#dict_dec['barely']= ['dec']\n",
    "dict_dec['little']= ['dec']\n",
    "#dict_dec['hardly']= ['dec']\n",
    "with open('dicts/f_inc.yml', 'w') as outfile:\n",
    "    yaml.dump(dict_dec, outfile, default_flow_style=True)\n",
    "\n",
    "dict_inv={}\n",
    "dict_inv['lack of']= ['inv']\n",
    "dict_inv['not']= ['inv']\n",
    "dict_inv['lack']= ['inv']\n",
    "with open('dicts/f_inv.yml', 'w') as outfile:\n",
    "    yaml.dump(dict_inv, outfile, default_flow_style=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "text = \"\"\"What can I say about this place. The staff of the restaurant is nice and the eggplant is not bad. Apart from that, very uninspired food, lack of atmosphere and too expensive. I am a staunch vegetarian and was sorely dissapointed with the veggie options on the menu. Will be the last time I visit, I recommend others to avoid.\"\"\"\n",
    "\n",
    "splitter = Splitter()\n",
    "postagger = POSTagger()\n",
    "\n",
    "splitted_sentences = splitter.split(text)\n",
    "pos_tagged_sentences = postagger.pos_tag(splitted_sentences)\n",
    "\n",
    "dicttagger = DictionaryTagger(['dicts/f_negative.yml','dicts/f_positive.yml','dicts/f_inc.yml','dicts/f_dec.yml','dicts/f_inv.yml'])\n",
    "dict_tagged_sentences = dicttagger.tag(pos_tagged_sentences)\n",
    "#print dict_tagged_sentences\n",
    "#print sentiment_score(dict_tagged_sentences)\n",
    "\n",
    "#print sentiment_score_basic_pos(dict_tagged_sentences)\n",
    "#print sentiment_score_basic_neg(dict_tagged_sentences)\n",
    "#print sentiment_score_basic(dict_tagged_sentences)\n",
    "print sentiment_score_pos(dict_tagged_sentences)\n",
    "print sentiment_score_neg(dict_tagged_sentences)\n",
    "#print dict_tagged_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "print sentiment_score_pos(dict_tagged_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print dict_tagged_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x=xrange(0,len(data))\n",
    "print x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
