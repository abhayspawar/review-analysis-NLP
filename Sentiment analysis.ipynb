{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once deleted, variables cannot be recovered. Proceed (y/[n])? y\n"
     ]
    }
   ],
   "source": [
    "%reset\n",
    "import nltk\n",
    "\n",
    "class Splitter(object):\n",
    "    def __init__(self):\n",
    "        self.nltk_splitter = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "        self.nltk_tokenizer = nltk.tokenize.TreebankWordTokenizer()\n",
    "\n",
    "    def split(self, text):\n",
    "        \"\"\"\n",
    "        input format: a paragraph of text\n",
    "        output format: a list of lists of words.\n",
    "            e.g.: [['this', 'is', 'a', 'sentence'], ['this', 'is', 'another', 'one']]\n",
    "        \"\"\"\n",
    "        sentences = self.nltk_splitter.tokenize(text)\n",
    "        tokenized_sentences = [self.nltk_tokenizer.tokenize(sent) for sent in sentences]\n",
    "        return tokenized_sentences\n",
    "\n",
    "\n",
    "class POSTagger(object):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "        \n",
    "    def pos_tag(self, sentences):\n",
    "        \"\"\"\n",
    "        input format: list of lists of words\n",
    "            e.g.: [['this', 'is', 'a', 'sentence'], ['this', 'is', 'another', 'one']]\n",
    "        output format: list of lists of tagged tokens. Each tagged tokens has a\n",
    "        form, a lemma, and a list of tags\n",
    "            e.g: [[('this', 'this', ['DT']), ('is', 'be', ['VB']), ('a', 'a', ['DT']), ('sentence', 'sentence', ['NN'])],\n",
    "                    [('this', 'this', ['DT']), ('is', 'be', ['VB']), ('another', 'another', ['DT']), ('one', 'one', ['CARD'])]]\n",
    "        \"\"\"\n",
    "\n",
    "        pos = [nltk.pos_tag(sentence) for sentence in sentences]\n",
    "        #adapt format\n",
    "        pos = [[(word, word, [postag]) for (word, postag) in sentence] for sentence in pos]\n",
    "        return pos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#from yaml import load, dump\n",
    "import yaml\n",
    "class DictionaryTagger(object):\n",
    "    def __init__(self, dictionary_paths):\n",
    "        files = [open(path, 'r') for path in dictionary_paths]\n",
    "            \n",
    "        dictionaries = [yaml.load(dict_file) for dict_file in files]\n",
    "        #dictionaries = [dict_positive,dict_negative]\n",
    "\n",
    "        map(lambda x: x.close(), files)\n",
    "        self.dictionary = {}\n",
    "        self.max_key_size = 0\n",
    "        for curr_dict in dictionaries:\n",
    "            for key in curr_dict:\n",
    "                if key in self.dictionary:\n",
    "                    self.dictionary[key].extend(curr_dict[key])\n",
    "                else:\n",
    "                    self.dictionary[key] = curr_dict[key]\n",
    "                    self.max_key_size = max(self.max_key_size, len(key))\n",
    "\n",
    "    def tag(self, postagged_sentences):\n",
    "        return [self.tag_sentence(sentence) for sentence in postagged_sentences]\n",
    "\n",
    "    def tag_sentence(self, sentence, tag_with_lemmas=False):\n",
    "        \"\"\"\n",
    "        the result is only one tagging of all the possible ones.\n",
    "        The resulting tagging is determined by these two priority rules:\n",
    "            - longest matches have higher priority\n",
    "            - search is made from left to right\n",
    "        \"\"\"\n",
    "        tag_sentence = []\n",
    "        N = len(sentence)\n",
    "        if self.max_key_size == 0:\n",
    "            self.max_key_size = N\n",
    "        i = 0\n",
    "        while (i < N):\n",
    "            j = min(i + self.max_key_size, N) #avoid overflow\n",
    "            tagged = False\n",
    "            while (j > i):\n",
    "                expression_form = ' '.join([word[0] for word in sentence[i:j]]).lower()\n",
    "                expression_lemma = ' '.join([word[1] for word in sentence[i:j]]).lower()\n",
    "                if tag_with_lemmas:\n",
    "                    literal = expression_lemma\n",
    "                else:\n",
    "                    literal = expression_form\n",
    "                if literal in self.dictionary:\n",
    "                    #self.logger.debug(\"found: %s\" % literal)\n",
    "                    is_single_token = j - i == 1\n",
    "                    original_position = i\n",
    "                    i = j\n",
    "                    taggings = [tag for tag in self.dictionary[literal]]\n",
    "                    tagged_expression = (expression_form, expression_lemma, taggings)\n",
    "                    if is_single_token: #if the tagged literal is a single token, conserve its previous taggings:\n",
    "                        original_token_tagging = sentence[original_position][2]\n",
    "                        tagged_expression[2].extend(original_token_tagging)\n",
    "                    tag_sentence.append(tagged_expression)\n",
    "                    tagged = True\n",
    "                else:\n",
    "                    j = j - 1\n",
    "            if not tagged:\n",
    "                tag_sentence.append(sentence[i])\n",
    "                i += 1\n",
    "        return tag_sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def value_of(sentiment):\n",
    "    if sentiment == 'positive': return 1\n",
    "    if sentiment == 'negative': return -1\n",
    "    return 0\n",
    "\n",
    "def value_of_pos(sentiment):\n",
    "    if sentiment == 'positive': return 1\n",
    "    return 0\n",
    "\n",
    "def value_of_neg(sentiment):\n",
    "    if sentiment == 'negative': return -1\n",
    "    return 0\n",
    "    \n",
    "def sentiment_score_basic_pos(review):    \n",
    "    return sum ([value_of_pos(tag) for sentence in dict_tagged_sentences for token in sentence for tag in token[2]])\n",
    "\n",
    "def sentiment_score_basic_neg(review):    \n",
    "    return sum ([value_of_neg(tag) for sentence in dict_tagged_sentences for token in sentence for tag in token[2]])\n",
    "\n",
    "def sentiment_score_basic(review):    \n",
    "    return sum ([value_of(tag) for sentence in dict_tagged_sentences for token in sentence for tag in token[2]])\n",
    "\n",
    "def sentence_score(sentence_tokens, previous_token, acum_score):    \n",
    "    if not sentence_tokens:\n",
    "        return acum_score\n",
    "    else:\n",
    "        current_token = sentence_tokens[0]\n",
    "        tags = current_token[2]\n",
    "        token_score = sum([value_of(tag) for tag in tags])\n",
    "        if previous_token is not None:\n",
    "            previous_tags = previous_token[2]\n",
    "            if 'inc' in previous_tags:\n",
    "                token_score *= 2.0\n",
    "            elif 'dec' in previous_tags:\n",
    "                token_score /= 2.0\n",
    "            elif 'inv' in previous_tags:\n",
    "                token_score *= -1.0\n",
    "        return sentence_score(sentence_tokens[1:], current_token, acum_score + token_score)\n",
    "\n",
    "def sentence_score_pos(sentence_tokens, previous_token, acum_score):    \n",
    "    if not sentence_tokens:\n",
    "        return acum_score\n",
    "        \n",
    "    else:\n",
    "        current_token = sentence_tokens[0]\n",
    "        tags = current_token[2]\n",
    "        token_score = sum([value_of(tag) for tag in tags])\n",
    "        \n",
    "        if previous_token is not None:\n",
    "            previous_tags = previous_token[2]\n",
    "            if 'inc' in previous_tags:\n",
    "                token_score *= 2.0\n",
    "            elif 'dec' in previous_tags:\n",
    "                token_score /= 2.0\n",
    "            elif 'inv' in previous_tags:\n",
    "                token_score *= -1.0\n",
    "        if token_score<0:\n",
    "            token_score=0\n",
    "        return sentence_score_pos(sentence_tokens[1:], current_token, acum_score+token_score)\n",
    "\n",
    "def sentence_score_neg(sentence_tokens, previous_token, acum_score):    \n",
    "    if not sentence_tokens:\n",
    "        return acum_score\n",
    "    else:\n",
    "        current_token = sentence_tokens[0]\n",
    "        tags = current_token[2]\n",
    "        token_score = sum([value_of_neg(tag) for tag in tags])\n",
    "        \n",
    "        if previous_token is not None:\n",
    "            previous_tags = previous_token[2]\n",
    "            if 'inc' in previous_tags:\n",
    "                token_score *= 2.0\n",
    "            elif 'dec' in previous_tags:\n",
    "                token_score /= 2.0\n",
    "            elif 'inv' in previous_tags:\n",
    "                token_score *= -1.0\n",
    "        if token_score>0:\n",
    "            token_score=0\n",
    "        return sentence_score_neg(sentence_tokens[1:], current_token, acum_score + token_score)\n",
    "\n",
    "def sentiment_score(review):\n",
    "    return sum([sentence_score(sentence, None, 0.0) for sentence in review])\n",
    "\n",
    "def sentiment_score_pos(review):\n",
    "    return sum([sentence_score_pos(sentence, None, 0.0) for sentence in review])\n",
    "\n",
    "def sentiment_score_neg(review):\n",
    "    return sum([sentence_score_neg(sentence, None, 0.0) for sentence in review])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "cats = ['Rooms', 'Date', 'Location', 'Service', 'Business service', 'Author', 'Check in / front desk', 'No. Helpful', \n",
    "        'Cleanliness', 'Content', 'Value', 'No. Reader', 'Overall','sentiment','score','score_basic_pos','score_basic_neg',\n",
    "        'score_basic','score_pos','score_neg','length']\n",
    "\n",
    "def getBlankFrame():\n",
    "    \n",
    "    data = pd.DataFrame(columns=cats)\n",
    "    \n",
    "    return data\n",
    "\n",
    "\n",
    "def addFileToData(filename, data):\n",
    "    intColumns = ['No. Reader', 'No. Helpful', 'Cleanliness','Check in / front desk', 'Value', 'Overall', 'Service', 'Business service', 'Rooms', 'Location']\n",
    "    characterThreshold = 60\n",
    "    with open(filename, 'r') as content_file:\n",
    "        content = content_file.read()\n",
    "     \n",
    "    reviews = content.split(\"\\n\\n\")\n",
    "    for r in reviews:\n",
    "        thisReview = pd.Series([None]*len(cats), cats)\n",
    "        splt = r.split(\"\\n\")\n",
    "        for s in splt:\n",
    "            for c in cats:\n",
    "                if \"<\"+c+\">\" in s:\n",
    "                    value = s.replace('<'+c+'>', '')\n",
    "                    if c in intColumns:\n",
    "                        value = int(value)\n",
    "                    if value == -1: #we dont want -1 as this is going to mess up averaging, take np.nan\n",
    "                        value = np.nan\n",
    "\n",
    "                    if c == \"Content\":\n",
    "                        value = value.lower()\n",
    "\n",
    "                    thisReview[c] = value\n",
    "        thisReview[\"score\"]=0\n",
    "        thisReview[\"score_basic_pos\"]=0\n",
    "        thisReview[\"score_basic_neg\"]=0\n",
    "        thisReview[\"score_basic\"]=0\n",
    "        thisReview[\"score_pos\"]=0\n",
    "        thisReview[\"score_neg\"]=0\n",
    "        thisReview[\"length\"]=0\n",
    "\n",
    "        if not thisReview[\"Content\"] == None and len(thisReview[\"Content\"]) > characterThreshold:\n",
    "            #only add if theres content and its long enough\n",
    "            data = data.append(thisReview, ignore_index=True)\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1458, 21)\n"
     ]
    }
   ],
   "source": [
    "data = pd.DataFrame(columns=cats)\n",
    "\n",
    "data = addFileToData('C:/Users/Abhay Pawar/Documents/GitHub/data/Review_Texts/hotel_72579.dat', data)\n",
    "data = addFileToData('C:/Users/Abhay Pawar/Documents/GitHub/data/Review_Texts/hotel_72572.dat', data)\n",
    "data = addFileToData('C:/Users/Abhay Pawar/Documents/GitHub/data/Review_Texts/hotel_73855.dat', data)\n",
    "data = addFileToData('C:/Users/Abhay Pawar/Documents/GitHub/data/Review_Texts/hotel_73821.dat', data)\n",
    "data = addFileToData('C:/Users/Abhay Pawar/Documents/GitHub/data/Review_Texts/hotel_73985.dat', data)\n",
    "data = addFileToData('C:/Users/Abhay Pawar/Documents/GitHub/data/Review_Texts/hotel_75662.dat', data)\n",
    "data = addFileToData('C:/Users/Abhay Pawar/Documents/GitHub/data/Review_Texts/hotel_76061.dat', data)\n",
    "data = addFileToData('C:/Users/Abhay Pawar/Documents/GitHub/data/Review_Texts/hotel_77638.dat', data)\n",
    "data = addFileToData('C:/Users/Abhay Pawar/Documents/GitHub/data/Review_Texts/hotel_80083.dat', data)\n",
    "data = addFileToData('C:/Users/Abhay Pawar/Documents/GitHub/data/Review_Texts/hotel_80087.dat', data)\n",
    "data = addFileToData('C:/Users/Abhay Pawar/Documents/GitHub/data/Review_Texts/hotel_80797.dat', data)\n",
    "data = addFileToData('C:/Users/Abhay Pawar/Documents/GitHub/data/Review_Texts/hotel_80808.dat', data)\n",
    "data = addFileToData('C:/Users/Abhay Pawar/Documents/GitHub/data/Review_Texts/hotel_80864.dat', data)\n",
    "data = addFileToData('C:/Users/Abhay Pawar/Documents/GitHub/data/Review_Texts/hotel_80912.dat', data)\n",
    "data = addFileToData('C:/Users/Abhay Pawar/Documents/GitHub/data/Review_Texts/hotel_80930.dat', data)\n",
    "\n",
    "print(data.shape)\n",
    "#data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'reasonably priced hotel in downtown seattle we stayed at the loyal inn because we wanted to be in downtown seattle, since we only had two nights there. it exceeded our expectations in many respects. the rooms were large and comfortable, with many extra amenities. the breakfast was very extensive, with many fresh fruits, eggs, breads and even waffles.the hotel staff was very friendly and helpful, especially with suggestions of places to see during our rather limited time in the city. (highlights were the columbia center, the second tallest building on the west coast, with marvelous views of the surrounding area, and also the pike street market.)we also appreciated the location, as it is within two blocks of the area of free bus transportation from early morning to late evening on all buses in the downtown area. this was a marvelous idea, and did much to cut down on traffic congestion.i plan to return to this hotel in the spring of 2009. '"
      ]
     },
     "execution_count": 352,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['Content'][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "G:\\Program Files\\Anaconda2\\lib\\site-packages\\ipykernel\\__main__.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "G:\\Program Files\\Anaconda2\\lib\\site-packages\\ipykernel\\__main__.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "G:\\Program Files\\Anaconda2\\lib\\site-packages\\ipykernel\\__main__.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "G:\\Program Files\\Anaconda2\\lib\\site-packages\\ipykernel\\__main__.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "G:\\Program Files\\Anaconda2\\lib\\site-packages\\ipykernel\\__main__.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "G:\\Program Files\\Anaconda2\\lib\\site-packages\\ipykernel\\__main__.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rooms</th>\n",
       "      <th>Date</th>\n",
       "      <th>Location</th>\n",
       "      <th>Service</th>\n",
       "      <th>Business service</th>\n",
       "      <th>Author</th>\n",
       "      <th>Check in / front desk</th>\n",
       "      <th>No. Helpful</th>\n",
       "      <th>Cleanliness</th>\n",
       "      <th>Content</th>\n",
       "      <th>...</th>\n",
       "      <th>No. Reader</th>\n",
       "      <th>Overall</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>score</th>\n",
       "      <th>score_basic_pos</th>\n",
       "      <th>score_basic_neg</th>\n",
       "      <th>score_basic</th>\n",
       "      <th>score_pos</th>\n",
       "      <th>score_neg</th>\n",
       "      <th>length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.0</td>\n",
       "      <td>Jan 7, 2009</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>JeanMars</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>a really good choice ! when returning from haw...</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>None</td>\n",
       "      <td>15.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.0</td>\n",
       "      <td>Jan 5, 2009</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>kareemtownes</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>everything you could hope for my friends and i...</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>None</td>\n",
       "      <td>16.5</td>\n",
       "      <td>17.0</td>\n",
       "      <td>-3.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>-2.5</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Nov 14, 2008</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>maestrolms</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>reasonably priced hotel in downtown seattle we...</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>None</td>\n",
       "      <td>11.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>Oct 28, 2008</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>seychgo</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>stay away!!!! not worth it... dirty, run down,...</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>None</td>\n",
       "      <td>-7.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>-14.0</td>\n",
       "      <td>-3.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>-14.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.0</td>\n",
       "      <td>Oct 27, 2008</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>fudgemaker</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>worth the cost the room that i was given had n...</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>None</td>\n",
       "      <td>5.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Rooms          Date  Location  Service  Business service        Author  \\\n",
       "0    5.0   Jan 7, 2009       5.0      5.0               5.0      JeanMars   \n",
       "1    5.0   Jan 5, 2009       5.0      4.0               5.0  kareemtownes   \n",
       "2    NaN  Nov 14, 2008       NaN      NaN               NaN    maestrolms   \n",
       "3    1.0  Oct 28, 2008       1.0      3.0               NaN       seychgo   \n",
       "4    4.0  Oct 27, 2008       3.0      4.0               NaN    fudgemaker   \n",
       "\n",
       "   Check in / front desk No. Helpful  Cleanliness  \\\n",
       "0                    4.0         NaN          5.0   \n",
       "1                    5.0         NaN          5.0   \n",
       "2                    NaN         NaN          NaN   \n",
       "3                    3.0         NaN          1.0   \n",
       "4                    2.0           1          4.0   \n",
       "\n",
       "                                             Content   ...    No. Reader  \\\n",
       "0  a really good choice ! when returning from haw...   ...           NaN   \n",
       "1  everything you could hope for my friends and i...   ...           NaN   \n",
       "2  reasonably priced hotel in downtown seattle we...   ...           NaN   \n",
       "3  stay away!!!! not worth it... dirty, run down,...   ...           NaN   \n",
       "4  worth the cost the room that i was given had n...   ...             1   \n",
       "\n",
       "  Overall  sentiment score  score_basic_pos  score_basic_neg  score_basic  \\\n",
       "0     5.0       None  15.0             10.0             -1.0          9.0   \n",
       "1     5.0       None  16.5             17.0             -3.0         14.0   \n",
       "2     5.0       None  11.0             11.0             -1.0         10.0   \n",
       "3     1.0       None  -7.0             11.0            -14.0         -3.0   \n",
       "4     3.0       None   5.0              7.0             -2.0          5.0   \n",
       "\n",
       "   score_pos  score_neg  length  \n",
       "0       16.0       -1.0     0.0  \n",
       "1       19.0       -2.5     0.0  \n",
       "2       12.0       -1.0     0.0  \n",
       "3        9.0      -14.0     0.0  \n",
       "4        7.0       -2.0     0.0  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 353,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dicttagger = DictionaryTagger(['dicts/f_negative.yml', 'dicts/f_positive.yml','dicts/f_inc.yml','dicts/f_dec.yml','dicts/f_inv.yml'])\n",
    "splitter = Splitter()\n",
    "postagger = POSTagger()\n",
    "\n",
    "for i in range(0,len(data)):\n",
    "    review=data['Content'][i].decode('utf-8')\n",
    "    splitted_sentences = splitter.split(review)\n",
    "    pos_tagged_sentences = postagger.pos_tag(splitted_sentences)\n",
    "    dict_tagged_sentences = dicttagger.tag(pos_tagged_sentences)\n",
    "    #data['sentiment'][i]=dict_tagged_sentences\n",
    "    data['score'][i]=sentiment_score(dict_tagged_sentences)\n",
    "    data[\"score_basic_pos\"][i]=sentiment_score_basic_pos(dict_tagged_sentences)\n",
    "    data[\"score_basic_neg\"][i]=sentiment_score_basic_neg(dict_tagged_sentences)\n",
    "    data[\"score_basic\"][i]=sentiment_score_basic(dict_tagged_sentences)\n",
    "    data[\"score_pos\"][i]=sentiment_score_pos(dict_tagged_sentences)\n",
    "    data[\"score_neg\"][i]=sentiment_score_neg(dict_tagged_sentences)\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-383-42c71844c85a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[1;31m#data['Overall'][data['Overall']!=0]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32mG:\\Program Files\\Anaconda2\\lib\\site-packages\\pandas\\core\\frame.pyc\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1995\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1996\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1997\u001b[0;31m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_column\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1998\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1999\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_getitem_column\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mG:\\Program Files\\Anaconda2\\lib\\site-packages\\pandas\\core\\frame.pyc\u001b[0m in \u001b[0;36m_getitem_column\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2002\u001b[0m         \u001b[1;31m# get column\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   2003\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_unique\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2004\u001b[0;31m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_item_cache\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2005\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   2006\u001b[0m         \u001b[1;31m# duplicate columns & possible reduce dimensionality\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mG:\\Program Files\\Anaconda2\\lib\\site-packages\\pandas\\core\\generic.pyc\u001b[0m in \u001b[0;36m_get_item_cache\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m   1348\u001b[0m         \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1349\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mres\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1350\u001b[0;31m             \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1351\u001b[0m             \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_box_item_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m             \u001b[0mcache\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mG:\\Program Files\\Anaconda2\\lib\\site-packages\\pandas\\core\\internals.pyc\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, item, fastpath)\u001b[0m\n\u001b[1;32m   3288\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   3289\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misnull\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3290\u001b[0;31m                 \u001b[0mloc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3291\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   3292\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0misnull\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mG:\\Program Files\\Anaconda2\\lib\\site-packages\\pandas\\indexes\\base.pyc\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   1945\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1946\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1947\u001b[0;31m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_cast_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1948\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1949\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtolerance\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas\\index.pyx\u001b[0m in \u001b[0;36mpandas.index.IndexEngine.get_loc (pandas\\index.c:4154)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas\\index.pyx\u001b[0m in \u001b[0;36mpandas.index.IndexEngine.get_loc (pandas\\index.c:4018)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas\\hashtable.pyx\u001b[0m in \u001b[0;36mpandas.hashtable.PyObjectHashTable.get_item (pandas\\hashtable.c:12368)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas\\hashtable.pyx\u001b[0m in \u001b[0;36mpandas.hashtable.PyObjectHashTable.get_item (pandas\\hashtable.c:12322)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 1"
     ]
    }
   ],
   "source": [
    "#data['Overall'][data['Overall']!=0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#'score','score_basic_pos','score_basic_neg',\n",
    "#        'score_basic','score_pos','score_neg',\n",
    "\n",
    "score=data['score'][data['Overall']!=0].reshape(-1,1)\n",
    "score_basic_pos=data['score_basic_pos'][data['Overall']!=0].reshape(-1,1)\n",
    "score_basic_neg=data['score_basic_neg'][data['Overall']!=0].reshape(-1,1)\n",
    "score_basic=data['score_basic'][data['Overall']!=0].reshape(-1,1)\n",
    "score_pos=data['score_pos'][data['Overall']!=0].reshape(-1,1)\n",
    "score_neg=data['score_neg'][data['Overall']!=0].reshape(-1,1)\n",
    "\n",
    "X=[]\n",
    "for i in range(0,len(score)):\n",
    "    #X.extend([[score[i][0],score_basic_pos[i][0],score_basic_neg[i][0],score_basic[i][0],score_pos[i][0],score_neg[i][0]]])\n",
    "    X.extend([[score[i][0]]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#'score','score_basic_pos','score_basic_neg',\n",
    "#        'score_basic','score_pos','score_neg',\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.svm import SVC\n",
    "clf = SVC()\n",
    "#clf.fit(X[0:1000],data['Overall'][0:1000])\n",
    "clf.fit(X,data['Overall'][data['Overall']!=0]) \n",
    "\n",
    "#score(X, y, sample_weight=None)\n",
    "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
    "    decision_function_shape=None, degree=3, gamma='auto', kernel='rbf',\n",
    "    max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
    "    tol=0.001, verbose=False)\n",
    "pred=clf.predict(X)\n",
    "#score(data['score'].reshape(-1, 1), data['Overall'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.454982817869\n",
      "0.699656357388\n"
     ]
    }
   ],
   "source": [
    "sum=float(0)\n",
    "error=float(0)\n",
    "#actual=data['Overall'].reshape(-1,1)[1001:len(X)]\n",
    "actual=data['Overall'][data['Overall']!=0].reshape(-1,1)\n",
    "\n",
    "#print len(pred)\n",
    "#print len(actual)\n",
    "for i in range(0,len(pred)):\n",
    "    if pred[i]==actual[i][0]:\n",
    "        sum+=1\n",
    "    error_curr=abs(actual[i][0]-pred[i])\n",
    "    error+=error_curr\n",
    "accuracy=sum/len(pred)\n",
    "mean_error=error/len(pred)\n",
    "\n",
    "print accuracy\n",
    "print mean_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#code to create file in desired format\n",
    "\n",
    "file=open('C:/Users/Abhay Pawar/Documents/GitHub/data/opinion-lexicon-English/positive-words.txt')\n",
    "lines=file.readlines()\n",
    "dict_positive={}\n",
    "for line in lines:\n",
    "    dict_positive[line.strip()]=['positive']\n",
    "with open('dicts/f_positive.yml', 'w') as outfile:\n",
    "    yaml.dump(dict_positive, outfile, default_flow_style=True)\n",
    "    \n",
    "file=open('C:/Users/Abhay Pawar/Documents/GitHub/data/opinion-lexicon-English/negative-words.txt')\n",
    "lines=file.readlines()\n",
    "dict_negative={}\n",
    "for line in lines:\n",
    "    dict_negative[line.strip()]=['negative']\n",
    "with open('dicts/f_negative.yml', 'w') as outfile:\n",
    "    yaml.dump(dict_negative, outfile, default_flow_style=True)\n",
    "\n",
    "dict_inc={}\n",
    "dict_inc['too']= ['inc']\n",
    "dict_inc['very']= ['inc']\n",
    "dict_inc['sorely']= ['inc']\n",
    "dict_inc['extremely']= ['inc']\n",
    "dict_inc['really']= ['inc']\n",
    "with open('dicts/f_dec.yml', 'w') as outfile:\n",
    "    yaml.dump(dict_inc, outfile, default_flow_style=True)\n",
    "\n",
    "dict_dec={}\n",
    "#dict_dec['barely']= ['dec']\n",
    "dict_dec['little']= ['dec']\n",
    "#dict_dec['hardly']= ['dec']\n",
    "with open('dicts/f_inc.yml', 'w') as outfile:\n",
    "    yaml.dump(dict_dec, outfile, default_flow_style=True)\n",
    "\n",
    "dict_inv={}\n",
    "dict_inv['lack of']= ['inv']\n",
    "dict_inv['not']= ['inv']\n",
    "dict_inv['lack']= ['inv']\n",
    "with open('dicts/f_inv.yml', 'w') as outfile:\n",
    "    yaml.dump(dict_inv, outfile, default_flow_style=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.0\n",
      "-5.0\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\"What can I say about this place. The staff of the restaurant is nice and the eggplant is not bad. Apart from that, very uninspired food, lack of atmosphere and too expensive. I am a staunch vegetarian and was sorely dissapointed with the veggie options on the menu. Will be the last time I visit, I recommend others to avoid.\"\"\"\n",
    "\n",
    "splitter = Splitter()\n",
    "postagger = POSTagger()\n",
    "\n",
    "splitted_sentences = splitter.split(text)\n",
    "pos_tagged_sentences = postagger.pos_tag(splitted_sentences)\n",
    "\n",
    "dicttagger = DictionaryTagger(['dicts/f_negative.yml','dicts/f_positive.yml','dicts/f_inc.yml','dicts/f_dec.yml','dicts/f_inv.yml'])\n",
    "dict_tagged_sentences = dicttagger.tag(pos_tagged_sentences)\n",
    "#print dict_tagged_sentences\n",
    "#print sentiment_score(dict_tagged_sentences)\n",
    "\n",
    "#print sentiment_score_basic_pos(dict_tagged_sentences)\n",
    "#print sentiment_score_basic_neg(dict_tagged_sentences)\n",
    "#print sentiment_score_basic(dict_tagged_sentences)\n",
    "print sentiment_score_pos(dict_tagged_sentences)\n",
    "print sentiment_score_neg(dict_tagged_sentences)\n",
    "#print dict_tagged_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print sentiment_score_pos(dict_tagged_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[('What', 'What', ['WP']), ('can', 'can', ['MD']), ('I', 'I', ['PRP']), ('say', 'say', ['VBP']), ('about', 'about', ['IN']), ('this', 'this', ['DT']), ('place', 'place', ['NN']), ('.', '.', ['.'])], [('The', 'The', ['DT']), ('staff', 'staff', ['NN']), ('of', 'of', ['IN']), ('the', 'the', ['DT']), ('restaurant', 'restaurant', ['NN']), ('is', 'is', ['VBZ']), ('nice', 'nice', ['positive', 'JJ']), ('and', 'and', ['CC']), ('the', 'the', ['DT']), ('eggplant', 'eggplant', ['NN']), ('is', 'is', ['VBZ']), ('not', 'not', ['inv', 'RB']), ('bad', 'bad', ['negative', 'JJ']), ('.', '.', ['.'])], [('Apart', 'Apart', ['RB']), ('from', 'from', ['IN']), ('that', 'that', ['IN']), (',', ',', [',']), ('very', 'very', ['inc', 'RB']), ('uninspired', 'uninspired', ['JJ']), ('food', 'food', ['NN']), (',', ',', [',']), ('lack of', 'lack of', ['inv']), ('atmosphere', 'atmosphere', ['NN']), ('and', 'and', ['CC']), ('too', 'too', ['inc', 'RB']), ('expensive', 'expensive', ['negative', 'JJ']), ('.', '.', ['.'])], [('I', 'I', ['PRP']), ('am', 'am', ['VBP']), ('a', 'a', ['DT']), ('staunch', 'staunch', ['positive', 'NN']), ('vegetarian', 'vegetarian', ['NN']), ('and', 'and', ['CC']), ('was', 'was', ['VBD']), ('sorely', 'sorely', ['negative', 'inc', 'RB']), ('dissapointed', 'dissapointed', ['negative', 'VBN']), ('with', 'with', ['IN']), ('the', 'the', ['DT']), ('veggie', 'veggie', ['NN']), ('options', 'options', ['NNS']), ('on', 'on', ['IN']), ('the', 'the', ['DT']), ('menu', 'menu', ['NN']), ('.', '.', ['.'])], [('Will', 'Will', ['MD']), ('be', 'be', ['VB']), ('the', 'the', ['DT']), ('last', 'last', ['JJ']), ('time', 'time', ['NN']), ('I', 'I', ['PRP']), ('visit', 'visit', ['VBP']), (',', ',', [',']), ('I', 'I', ['PRP']), ('recommend', 'recommend', ['positive', 'VBP']), ('others', 'others', ['NNS']), ('to', 'to', ['TO']), ('avoid', 'avoid', ['VB']), ('.', '.', ['.'])]]\n"
     ]
    }
   ],
   "source": [
    "print dict_tagged_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xrange(1458)\n"
     ]
    }
   ],
   "source": [
    "x=xrange(0,len(data))\n",
    "print x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
